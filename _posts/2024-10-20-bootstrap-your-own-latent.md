---
layout: post
title: "Bootstrap Your Own Latent: Self-Supervised Learning Without 
Contrastive Learning"
author: Pedro Tajia
tags: [Self Supervised, Deep Learning, ]
---
## Introduction
Supervised Learning are top solution to train Convolutional Neural Network (CNN), which are model made to solve computer vision like self-driving car, security, automatization, etc. As these computer vision tasks increases the need to improve these systems also emerges.

One of the main ways to improve these systems is through having more data and bigger model, but unfortunately by using supervised learning these solutions need labeled data (information that have been classified by humans), which makes them expensive and difficult to get.

The bottleneck that limits the process of training is the process itself. Since supervised learning is dependent on label data which limits the amount of information the model are trained on. This represents a big limitation on the training of these model and a waste of unlabeled data. Current research on Self-Supervised Learning has able to train model without label data and take out the need of using Contrastive Learning which are the most common way to train model with Self-Supervised Learning. 


