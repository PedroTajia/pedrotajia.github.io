<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://pedrotajia.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://pedrotajia.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-31T00:02:34+00:00</updated><id>https://pedrotajia.com/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">AI Can Learn From Their Dreams: World Models</title><link href="https://pedrotajia.com/blog/2025/ai-can-learn-from-their-dreams-world-models/" rel="alternate" type="text/html" title="AI Can Learn From Their Dreams: World Models"/><published>2025-05-21T23:18:40+00:00</published><updated>2025-05-21T23:18:40+00:00</updated><id>https://pedrotajia.com/blog/2025/ai-can-learn-from-their-dreams-world-models</id><content type="html" xml:base="https://pedrotajia.com/blog/2025/ai-can-learn-from-their-dreams-world-models/"><![CDATA[<p>AI Can Learn From Their Dreams: World Models</p> <p>By: Pedro Tajia &amp; Syed Islam</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*X4f1INi-khcMteZ5"/></figure> <h3>Introduction:</h3> <p>Since birth, humans have functioned as information processing machines through our senses with vision, audio, touch, etc. We learn to process this information naturally, analyzing and abstracting it to build what we call our “world model” or our understanding of “reality.” Our world model emerges from these limited senses and shapes how we perceive everything around us.</p> <p>Most people don’t directly process complex events like astronaut activities on the International Space Station. Instead, they build their reality through information from others who help make sense of these distant happenings. Think about reading this text right now — maybe you’re in your bedroom looking at your phone screen. This moment forms your current reality, and when you notice a car passing by your window, your mind naturally adds this new detail to your understanding. This mental picture of reality that grows in your mind becomes your personal “World Model.”</p> <p>Now imagine taking this idea of how we create and update our world model and applying it to AI, especially in Deep Learning systems. What possibilities might that open up? Well in this Article, we will take a deeper look into how we can apply the “World Model” idea into the world of AI.</p> <h3>World model in deep learning:</h3> <p>In order to apply the idea of the world model in deep learning (a subfield of AI that uses neural networks), the system has to learn about its environment.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MupvOfr1JE3HzanI"/></figure> <p>Figure 1: A single frame from the car simulator (<a href="https://gymnasium.farama.org/environments/box2d/car_racing/">CarRacing-v3</a>).</p> <p>We want the model to understand the physics and behavior of this simulator. In the paper, “<a href="https://arxiv.org/abs/1803.10122">World Models</a>”, it shows that with a variational autoencoder (VAE) we can approximate this simulation.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Ih3dyECBxPhZBVkO"/></figure> <p>Figure 2: This image shows VAE, an unsupervised deep learning algorithm used to learn representations from the data. Given an input, the encoder reduces the information into a vector representation and the decoder recovers the information from the vector into a reconstructed image.</p> <p>To train this VAE to gain an understanding of the environment the agent (the entity that interacts in the environment) a.k.a “the red car” collects the data. The agents act randomly to explore the environment for each action at and the resultant observation will be stored to train this VAE. 10,000 rollouts each rollout is the agent interacting in the environment where the agent is in the race for some time and then terminates.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qnd6GkLQUQhtYPCO"/></figure> <p>Figure 3: A sequence of images from the car simulation with its <strong>action a </strong>at a <strong>time t </strong>or <strong>frame</strong>.</p> <p>This is the list of discrete actions available in the car simulation [ 0: do nothing, 1: steer left, 2: steer right, 3: gas, 4: brake]</p> <p>These frame observations are used to train the VAE to make the encoder represent each frame into a vector representation <em>z</em>, and then the decoder uses this vector <em>z</em> to reconstruct the original frame. The task of the VAE is to reduce the difference between the original and reconstructed frame, whereby doing this we can have correct vector representations of the observations/frames.</p> <p>When the VAE is already trained on the rollouts, we will use the <strong>encoder </strong>to produce vector representations z to train a Recurrent Neural Network(RNN), which serves as a predictive model for the future zt+1 vector which is put together with a Mixture Density Network to give this next z vector. With this, the RNN will generate the next vector zt+1 given the action at taken from the vector representation, and with the hidden state of the RNN ht more formally P(zt+1| at, zt, ht).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rrCzwiWAYbnEaJ6p"/></figure> <p>Figure 4: A figure of the <a href="https://arxiv.org/pdf/1803.10122">original paper</a>. The RNN with the MDN (MDN-RNN) will generate a vector zt+1 from the past zt vector, action, at and hidden state ht.</p> <p>As the model learns to predict the next vector representation zt+1 given zt, at and ht the model will learn to analyze the past, understand the present, and be able to predict the future.</p> <p>The last part of the world model system is the controller or agent, which is the one that gives the hidden state at time t of an RNN ht and the vector z from the encoder given the observation the controller will perform some action at. The original paper explains that the controller is the smallest part of the world model, only having a small number of parameters as compared to the VAE and MDN-RNN, because the controller works in the vector space z and not on the observation, which makes it possible to have a small controller since it will not work in understanding the meaning of an image as the VAE does. The controller can be viewed as a linear function having at = Wc [zt , ht] + bc where Wc and bc are learnable parameters are learning by taking the action at given zt and ht that maximizes the expected cumulative reward.</p> <p>By combining the VAE, MDR-RNN, and the Controller, we will have the <strong>World Model</strong>. When all the parts of the world model are trained to give a single frame, we can generate a sequence of observations that the Controller can interact with, which means that the controller can learn from the generated vector representations from the MDR-RNN and can be seen from the decoder of the VAE.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*O7CELSjNzwID_2bR"/></figure> <p>Figure 5: This figure shows a small sequence of dreamed examples. The first image is gathered from the original simulation and passed to an encoder, which is transferred into a latent vector. This vector is inputted to the <strong>controller C</strong>, taking action in this deemed simulation. The action is passed to the <strong>MDN-RNN</strong> where it returns a predicted reward and latent vector which the controller can use to take an action and improve itself by the given reward. This sequence of actions is finished when the controller decides to finish the simulation.</p> <h3>Benefits of the Word Model</h3> <p>Now that we have a good understanding on how World Models function in AI, let’s take a look at how they benefit us. The core advantage is pretty straightforward — World Models create simplified training environments that dramatically reduce computational requirements.</p> <p>For example, instead of training our AI agents directly with high-resolution images, World Models convert these images into what we call latent vectors, which are compressed versions of what the AI sees. Think of it like this: rather than dealing with every single pixel of a 4K image (which would be very computationally expensive), the World Model boils it down to just the essential information the AI needs.</p> <p>But here’s where it gets more beneficial — World Models can handle all sorts of different environments, not just visual ones. Want to simulate how a robot should move? World Models can help with that. Need to model complex business decisions? They’ve got you covered there. Even abstract problem-solving scenarios can be modeled this way. In each case, the World Model takes complicated information and turns it into a simplified version that’s much easier to work with while keeping all the important stuff needed for training intact.</p> <p>This versatility is what makes World Models so advantageous. And to see just how effective they can be, let’s take a look at the VizDoom experiment.</p> <h3>Application of a World Model (VizDoom Experiment)</h3> <p>The VizDoom experiment offers compelling evidence of World Models’ effectiveness. In this experiment, researchers trained an AI agent in a dream-like environment that simulated the VizDoom game, with the primary goal of determining whether skills learned in a simulated environment could transfer successfully to the actual game.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*0vzo6GF1b5uNiaT8"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9uu6VOQ2GGg-3uJq"/></figure> <p>Figure 6: Actual Game Environment Figure 7: Dream-like Environment</p> <p>The experiment’s setup was straightforward: the AI agent was placed in a closed room where it needed to dodge fireballs shot by monsters, with the objective of surviving as long as possible. What made the results particularly interesting was how well the agent adapted to its environment. Through training in the dream world, the agent developed realistic behaviors, learning to navigate without hitting the walls and avoid incoming fireballs effectively.</p> <p>Perhaps most remarkably, the researchers discovered that by increasing certain parameters to make the dream environment more challenging than the real game — thus making it harder for the agent to dodge fireballs — the agent’s performance in the actual game environment improved significantly. This finding demonstrates a key advantage of World Models: not only can they train agents with fewer computational resources, but they can potentially achieve better results than training in the real environment itself.</p> <p>This experiment serves as a powerful demonstration of World Models’ potential, showing that effective training can occur in simplified, dream-like environments while still producing superior real-world results.</p> <h3>Conclusion</h3> <p>Throughout this article, we explored the concept of World Models applied to AI. This exploration reveals a fundamental shift in how AI might learn and understand its environment. Just as humans build mental models of reality from their limited sensory inputs, these AI systems can now construct their own representations of the world around them. Through the combination of VAEs, MDN-RNNS, and simple controllers, we’ve shown that AI can not only perceive its environment but create internal simulations to learn from.</p> <p>However, there are still significant challenges that remain. Our current AI systems, despite their amazing capabilities, are constrained by memory limitations. Also, agents might exploit imperfections in their world model, achieving high scores in simulated environments that don’t translate to real-world success.</p> <p>The implications of the world model stretch beyond just technical achievements. World models could change AI development by lowering computational requirements and making advanced training methods accessible to smaller organizations. This shift could spark innovation across industries, create new opportunities in simulation-based training systems and complex decision-making applications.</p> <p>The idea of the world model prompts a deeper philosophical question about the nature of intelligence and consciousness. When an AI system can “dream” and learn from imagined scenarios, what does that tell us about the nature of understanding and learning? How do we ensure that behaviors learned in simulated environments align with real-world requirements?</p> <p>As we move forward, the key lies in balancing rapid advancement with responsible developments. World models represent more than just a technical achievement — they signal a fundamental shift in how AI might learn and understand its environment. The challenge ahead lies in nurturing growth while ensuring it remains aligned with human values and societal benefit.</p> <p>Source Links:</p> <ul><li><a href="https://medium.com/@barrettnash/deepseeks-disruptive-debut-true-capitalism-in-action-sorry-trillion-dollar-oligarchs-cc82f536d544">https://medium.com/@barrettnash/deepseeks-disruptive-debut-true-capitalism-in-action-sorry-trillion-dollar-oligarchs-cc82f536d544</a></li><li><a href="https://www.bbc.com/news/articles/c5yv5976z9po">https://www.bbc.com/news/articles/c5yv5976z9po</a></li><li><a href="https://www.politico.com/newsletters/digital-future-daily/2025/01/27/whats-behind-the-deepseek-freakout-00200813">https://www.politico.com/newsletters/digital-future-daily/2025/01/27/whats-behind-the-deepseek-freakout-00200813</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3018fb21602b" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">AI &amp;amp; ML in Autonomous Driving</title><link href="https://pedrotajia.com/blog/2025/ai-ml-in-autonomous-driving/" rel="alternate" type="text/html" title="AI &amp;amp; ML in Autonomous Driving"/><published>2025-05-10T06:44:35+00:00</published><updated>2025-05-10T06:44:35+00:00</updated><id>https://pedrotajia.com/blog/2025/ai--ml-in-autonomous-driving</id><content type="html" xml:base="https://pedrotajia.com/blog/2025/ai-ml-in-autonomous-driving/"><![CDATA[<p>By: Pedro Tajia &amp; Riley Menter</p> <p><strong>Introduction</strong></p> <p>Artificial intelligence (AI) and Machine Learning (ML) are becoming increasingly prevalent in the automotive industry. Their applications are revolutionizing traditional manufacturing methods and fostering advancements in predictive maintenance, allowing vehicles to predict and reduce mechanical issues. This enables vehicles to be safer, more cost-effective, and more reliable.</p> <p>However, the most well-known applications of AI and ML in the automotive industry are the developments seen in autonomous driving. Vehicles with autonomous features today are considered to have advanced driver-assistance systems (ADAS). Despite being referred to as “self-driving,” these vehicles are not fully autonomous and are mainly limited to navigation features such as lane keeping, cruise control, and assisted parking.</p> <p>The most significant advantage of autonomous features is the potential to improve road safety. Autonomous driving technologies can reduce accidents by detecting and reacting to dangers faster than human drivers. Additionally, if an accident occurs, data collected by these systems, such as images, can be used for insurance claims, reducing discrepancies and allowing for clearer incident assessments.</p> <p><strong>History</strong></p> <p>The earliest signs of AI and ML in the automotive industry date back to around the mid-20th century. Cruise control and anti-lock braking systems are early examples of such technologies created to achieve safer and more effective transportation. However, it wasn’t until the late 20th and early 21st centuries that AI and ML began significantly impacting the industry. The key to this shift is innovations in computer vision, a branch of AI that allows computers to interpret images and videos, recognize patterns from them, and make decisions. ADAS uses computer vision to detect pedestrians, signs, and lane markings. When the information is interpreted, ADAS makes necessary adjustments to vehicle manoeuvres.</p> <p>In recent decades, ML algorithms have been crucial in developing autonomous features. These systems rely on these algorithms to process large amounts of data for decision-making and navigation. This is most notably seen at large companies like Tesla and Google, which invest heavily in these areas. Although it wasn’t the first form of autonomous driving in vehicles, Tesla popularized the autopilot concept in 2014.</p> <p>Today, many major automakers are working on implementing autonomous features. The National Highway Traffic Safety Administration has a 6-level ranking for cars with autonomous capabilities. ADAS systems fall between levels 0 and 2, requiring minimal human input when driving. Level 3 requires less, but levels 4 and 5 are fully autonomous vehicles. Nevertheless, even at Level 5, the driver must remain ready to take control.</p> <p><strong>Components of Self-Driving Vehicles</strong></p> <p>Vehicles with self-driving capabilities can make real-time decisions and navigate roads with minimal human input. This is achieved through decision-making algorithms that handle inputs from cameras, lidar, radar, and ultrasonic sensors. Combined, these features allow the vehicle to identify lines on the road (lanes), objects, and signs, enabling informed navigation decisions.</p> <p>Light Detection and Ranging (LiDAR) is a system that uses lasers to identify objects and gauge distances. A computer integrated inside the vehicle creates a “point cloud,” a 3D map of the surroundings consisting of discrete points with X, Y, and Z coordinates. This allows autonomous systems to identify objects on the road, such as other cars, people, animals, and obstacles. However, since LiDAR has limitations, cameras are used to classify objects and identify road markings, traffic signs, and lanes.</p> <p>Electronic Control Units (ECUs) and Onboard High-Performance Computers (HPCs) are also critical components. ECUs are small computers that manage specific vehicle parts, like brakes, engines, and transmissions, by processing sensor data. However, too many ECUs can create latency and spatial challenges. HPCs alleviate these issues by consolidating data processing across multiple ECUs, reducing workload, and ensuring the vehicle makes decisions quicker than human drivers.</p> <p>Finally, there is the AI and ML component. ML algorithms interpret the sensor data and make navigational decisions. These algorithms must accurately identify and differentiate objects to ensure safe driving. Continuous training is necessary to improve recognition of new or uncommon objects. Poorly designed algorithms can lead to misidentifications, resulting in unsafe behaviors and potential accidents. Therefore, strong algorithm development is crucial, and ongoing training is critical to creating safe and reliable autonomous vehicles.</p> <p><strong>Training Autonomous Vehicles: GAIA</strong></p> <p>In developing autonomous features, acquiring sufficient data to train these systems remains a major challenge. Building effective autonomous capabilities requires the system to fully understand its environment and make the correct real-time decisions. To help these systems learn, developers have turned to realistic simulations where the AI can interact with virtual environments and receive feedback on its actions. Simulated learning is highly beneficial because it eliminates the risks associated with real-world testing, where mistakes could cause damage to vehicles or harm people. Additionally, simulation allows accelerated learning — what would take an hour in real life can be compressed into the equivalent of a year’s experience for the autonomous system.</p> <p>Although simulation is advantageous in many areas, it also comes with limitations. Simulated environments cannot fully capture all the complexities and unpredictability of the real world. While simulations are useful for training systems with massive amounts of data, they are less reliable for evaluating whether an autonomous driving system will perform safely and efficiently outside the virtual world. For evaluation purposes, it is critical to have simulators that can accurately replicate real-world dynamics.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*n3FIRd5JdkGg0fyw"/></figure> <p><strong>Image 1</strong>: Comparison between a world model and a computer simulator for training autonomous vehicle systems.</p> <p>Wayve is a startup focused on developing AI systems for autonomous driving. They introduced GAIA-2, the successor to their earlier model GAIA-1, a world model designed to generate synthetic scenarios for training and validating autonomous systems. GAIA-2 is trained using camera data collected from vehicles across three different countries, enabling the creation of high-resolution, temporally consistent scenarios across multi-camera video sequences. These synthetic environments allow autonomous systems to test and improve their safety performance.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2yZITotvpitNP2qw"/></figure> <p><strong>Image 2</strong>: Example output from GAIA-2 for the first three cameras of an autonomous vehicle. <a href="https://wayve.ai/thinking/gaia-2/">Source</a></p> <p><strong>Limitations</strong></p> <p>From a technical perspective, AI isn’t perfect. Despite having advanced sensors and technology, autonomous features often struggle to handle out-of-the-ordinary driving situations. Varying weather conditions, road closures, and faded markings on the road can lead to undesired and dangerous navigation decisions. AI models are also computationally expensive, requiring large datasets to function effectively in real-time. The continuation of technological advancement. Technological limitations can be countered through continuous research and development work. Better decision-making algorithms and technologies can yield more effective and reliable autonomous systems.</p> <p>Ethics is also a concern in the development of autonomous systems. Let’s consider a situation where an accident is inevitable. Ethical dilemmas arise because the model is programmed to make instantaneous decisions. This brings about the question: whose safety should be prioritized? Should the vehicle’s occupants take priority? Or should it be other pedestrians or vehicles? Depending on the model’s decision, people’s lives can be put at risk.</p> <p>This further leads to more ethical issues concerning accountability, liability, and transparency in the decision-making algorithms that autonomous models use. Models must be designed with algorithms that adhere to legal and ethical responsibilities under challenging situations where accidents are inevitable. Additionally, since these models require vast amounts of data for model training, concerns regarding data privacy and cybersecurity also emerge.</p> <p>Ethical concerns can be mitigated through collaboration between policymakers, businesses, and engineers. Establishing clear guidelines, standards, and regulations allows for more ethically sound developments for autonomous driving. Lastly, educational incentives should be put in place to familiarize society with the abilities and limitations of autonomous vehicles.</p> <p><strong>Key takeaways</strong></p> <p>Autonomous driving systems have created new opportunities for research and development. Designing machines capable of making driving decisions is a major challenge. Still, through extensive research, it is possible to develop systems that can drive on streets, highways, and even in cities with minimal human input. The integration of sensors has enabled AI systems to gather critical information needed to make informed decisions. Sensors like LiDAR and cameras help create 3D representations of the environment, allowing the AI to predict future events and detect vehicles, pedestrians, traffic lights, road markings, and more. Meanwhile, components like ECUs and HPCs provide real-time data on the vehicle’s internal systems, such as the battery, engine, and transmission. This information allows the AI to make optimized decisions based on the vehicle’s condition, for example, suggesting nearby charging stations when battery levels are low and planning energy-efficient routes.</p> <p>Training AI systems directly in the real world is highly dangerous, as an untrained AI would initially take random and unsafe actions. To address this, researchers have turned to simulation, allowing AI systems to learn safely before being deployed in real-world vehicles. Simulations offer a safer and faster learning environment, compressing years of driving experience into hours. However, simulated environments sometimes fail to perfectly mimic real-world dynamics, creating gaps between simulation and reality. Simulations are excellent for large-scale data generation, but for final safety validation, ultra-realistic simulators are critical. To address this need, Wayve developed GAIA-2, a system that generates highly realistic scenarios to train and test autonomous systems. Despite the immense progress made, autonomous systems still face limitations. AI can occasionally make imperfect decisions that lead to dangerous situations. Ethical challenges also arise, particularly around in understanding AI’s decision-making process. To address these issues, guidelines, standards, and regulations are being established to ensure the development of safer and more trustworthy autonomous systems.</p> <p><strong>Works Cited:</strong></p> <ol><li><em>Artificial intelligence (AI) in the automotive industry ‒ Intel</em>. (n.d.). Intel. <a href="https://www.intel.com/content/www/us/en/learn/ai-in-automotive.html">https://www.intel.com/content/www/us/en/learn/ai-in-automotive.html</a></li><li>Hitchcock, S. (2024, September 29). <em>The rise of AI in the automotive industry</em>. WDA Automotive Marketing. <a href="https://wda-automotive.com/the-rise-of-artificial-intelligence-in-the-automotive-industry/">https://wda-automotive.com/the-rise-of-artificial-intelligence-in-the-automotive-industry/</a></li><li><em>How do Self-Driving cars work?</em> (2024, December 9). NI. <a href="https://www.ni.com/en/solutions/transportation/adas-and-autonomous-driving-testing/self-driving-cars-road-ahead-autonomous-vehicles.html?srsltid=AfmBOooMp3Qno413XjoS00EgKr6QUwAUzomXo9ygVBEasL6skdYon-O4">https://www.ni.com/en/solutions/transportation/adas-and-autonomous-driving-testing/self-driving-cars-road-ahead-autonomous-vehicles.html?srsltid=AfmBOooMp3Qno413XjoS00EgKr6QUwAUzomXo9ygVBEasL6skdYon-O4</a></li><li>Epicor, V. T. (n.d.). <em>The impact of AI in the automotive industry</em>. CDOTrends. <a href="https://www.cdotrends.com/story/15326/impact-ai-automotive-industry?utm_source=chatgpt.com">https://www.cdotrends.com/story/15326/impact-ai-automotive-industry?utm_source=chatgpt.com</a></li><li><em>Autonomous Driving: Pros &amp; Cons | SWARCO</em>. (n.d.). SWARCO. <a href="https://www.swarco.com/mobility-future/autonomous-driving/autonomous-driving-pros-cons">https://www.swarco.com/mobility-future/autonomous-driving/autonomous-driving-pros-cons</a></li><li>Rankin, R. (2025, March 27). <em>GAIA-2: Pushing the boundaries of video generative models for safer assisted and automated driving — WayVE</em>. Wayve. <a href="https://wayve.ai/thinking/gaia-2/">https://wayve.ai/thinking/gaia-2/</a></li><li>Gautam, A., Tiwari, D. R., &amp; IRJET. (2024). FUTURE OF ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING IN AUTOMOTIVE INDUSTRY: A REVIEW. <em>International Research Journal of Engineering and Technology</em>, <em>11–06</em>, 177–178. <a href="https://www.irjet.net/archives/V11/i6/IRJET-V11I630.pdf">https://www.irjet.net/archives/V11/i6/IRJET-V11I630.pd</a>f</li></ol> <p><strong>Links:</strong></p> <ul><li><strong>General Info: </strong><a href="https://www.intel.com/content/www/us/en/learn/ai-in-automotive.html"><strong>https://www.intel.com/content/www/us/en/learn/ai-in-automotive.html</strong></a></li><li><strong>History: </strong><a href="https://wda-automotive.com/the-rise-of-artificial-intelligence-in-the-automotive-industry/"><strong>https://wda-automotive.com/the-rise-of-artificial-intelligence-in-the-automotive-industry/</strong></a></li><li><strong>How it works: </strong><a href="https://www.ni.com/en/solutions/transportation/adas-and-autonomous-driving-testing/self-driving-cars-road-ahead-autonomous-vehicles.html?srsltid=AfmBOooMp3Qno413XjoS00EgKr6QUwAUzomXo9ygVBEasL6skdYon-O4"><strong>https://www.ni.com/en/solutions/transportation/adas-and-autonomous-driving-testing/self-driving-cars-road-ahead-autonomous-vehicles.html?srsltid=AfmBOooMp3Qno413XjoS00EgKr6QUwAUzomXo9ygVBEasL6skdYon-O4</strong></a></li><li><strong>General Info: </strong><a href="https://www.cdotrends.com/story/15326/impact-ai-automotive-industry?utm_source=chatgpt.com"><strong>https://www.cdotrends.com/story/15326/impact-ai-automotive-industry?utm_source=chatgpt.com</strong></a></li><li><strong>Limitations: </strong><a href="https://www.swarco.com/mobility-future/autonomous-driving/autonomous-driving-pros-cons"><strong>https://www.swarco.com/mobility-future/autonomous-driving/autonomous-driving-pros-cons</strong></a></li><li><strong>GAIA: </strong><a href="https://wayve.ai/thinking/introducing-gaia1/"><strong>https://wayve.ai/thinking/introducing-gaia1/</strong></a></li><li><strong>History, Ethics: </strong><a href="https://www.irjet.net/archives/V11/i6/IRJET-V11I630.pdf"><strong>https://www.irjet.net/archives/V11/i6/IRJET-V11I630.pdf</strong></a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3fbb992dcfc4" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Byte-Sized AI 2</title><link href="https://pedrotajia.com/blog/2025/byte-sized-ai-2/" rel="alternate" type="text/html" title="Byte-Sized AI 2"/><published>2025-04-30T00:57:05+00:00</published><updated>2025-04-30T00:57:05+00:00</updated><id>https://pedrotajia.com/blog/2025/byte-sized-ai-2</id><content type="html" xml:base="https://pedrotajia.com/blog/2025/byte-sized-ai-2/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pG6ebR7RXz7XVDTnLF8tYw.png"/></figure> <p>By: Sahana Narayan</p> <p>Welcome to Byte-Sized AI! Your biweekly update on everything AI! Artificial intelligence continues to grow at an extraordinary pace, embedding itself into nearly every sector of today’s world. March and April’s developments underscore the rapid evolution of the field, highlighting both the emergence of new AI agents and the challenges faced by existing ones.</p> <h3><strong>A New Trend in AI Stocks</strong></h3> <p>Amid Trump-era tariffs and ongoing stock market volatility, many are left questioning how these economic shifts might affect artificial intelligence’s evolving role in the market. As AI companies race to release new models and agents, it’s clear that AI-related stocks are under more scrutiny than ever before.</p> <p>Startups like OpenAI — recently valued at $40 billion — are making waves, signaling strong investor confidence in emerging AI technologies. However, according to <em>Investor’s Business Daily</em>, “One view is that software companies will emerge as the best AI stocks if they can monetize new products and services.” This suggests a shift in focus: rather than startups centered around building AI infrastructure, companies that use AI strategically to drive revenue may have the upper hand when it comes to securing capital. Reflecting this trend, many firms are pivoting toward AI-powered optimization, aiming to “boost productivity by developing customized AI for specific industries.”</p> <p>This dynamic raises an important question: will established software companies ultimately eclipse AI-first startups, or can the latter still lead the charge? OpenAI’s massive funding round indicates that the startup sector is far from out of the race. Yet moving forward, what matters most is not which companies talk the most about AI — but which ones effectively harness it to enhance their products.</p> <h3><strong>Artificial Intelligence: Your New Therapist?</strong></h3> <p>In the rapid development of AI, it is not uncommon for new agents to pop up, aiming to replace tasks that are done in regular daily life. Recent research suggests that “given the right kind of training, AI bots can deliver mental health therapy with as much efficacy as — or more than — human clinicians,” according to NPR.</p> <p>In <em>NEJM AI</em>, a medical journal, a study on AI therapy reported the results of the first randomized clinical trial. Amidst a shortage of mental health providers, researchers from Dartmouth College trained an AI bot to help address the gap in care. The study involved 200 participants, with half receiving treatment from AI therapy bots and the other half receiving none. “Compared to those that did not receive treatment, those who did showed significant improvement.” The study also revealed a surprising emotional connection between patients and the bot.</p> <p>Although the American Psychological Association currently prohibits the use of AI bots for mental health care, the volume of clinical research and promising results from this trial suggest that the field is heading in a promising direction. The growing body of evidence signals a shift in how we might supplement or support traditional therapy in the future.</p> <h3>Microsoft 365 Copilot’s New Agents</h3> <p>Microsoft 365 Copilot is an AI assistant designed to support a wide range of tasks across Microsoft applications, from Word to Excel. Recently, Microsoft expanded its Copilot suite with the introduction of two new agents — Researcher and Analyst — each tailored to handle complex information and data analysis.</p> <p>The Researcher agent “combines OpenAI’s deep research model with Microsoft 365 Copilot’s advanced orchestration and deep search capabilities,” according to Microsoft. It can build strategies based on collected data, help ideate and refine products, and even generate comprehensive reports. Meanwhile, the Analyst agent lives up to its name by transforming raw data into meaningful insights within minutes. “Analyst uses chain-of-thought reasoning to progress through problems iteratively, taking as many steps as necessary to refine its reasoning and provide a high-quality answer that mirrors human analytical thinking.”</p> <p>Together, Researcher and Analyst signal a larger trend in Microsoft’s roadmap: a strong commitment to embedding artificial intelligence wherever possible to enhance the efficiency and capabilities of their tools.</p> <h3>Behind OpenAI’s Funding</h3> <p>OpenAI has been on the rise as the forefront of the artificial intelligence landscape, and its recent funding has proven its mark on the field. Securing a staggering $40 billion in funding, this marks “the largest private tech fundraise in history.”</p> <p>The funding round was led by SoftBank, a Japanese investment giant known for backing major technology companies through its Vision Fund — a venture capital fund that targets global tech innovators like OpenAI. Other major contributors included Microsoft, Coatue, Altimeter, and Thrive Capital. In total, the $40 billion raise brings OpenAI’s valuation to an impressive $300 billion. According to CNBC, about $18 million of that funding will be allocated to a project called Stargate, which focuses on building advanced AI infrastructure.</p> <p>This historic investment not only cements OpenAI’s dominance in the AI space but also signals the growing global commitment to artificial intelligence.</p> <h3>Sources:</h3> <ol><li><a href="https://www.investors.com/news/technology/artificial-intelligence-stocks/">https://www.investors.com/news/technology/artificial-intelligence-stocks/</a></li><li><a href="https://www.npr.org/sections/shots-health-news/2025/04/07/nx-s1-5351312/artificial-intelligence-mental-health-therapy">https://www.npr.org/sections/shots-health-news/2025/04/07/nx-s1-5351312/artificial-intelligence-mental-health-therapy</a></li><li><a href="https://www.microsoft.com/en-us/microsoft-365/blog/2025/03/25/introducing-researcher-and-analyst-in-microsoft-365-copilot/">https://www.microsoft.com/en-us/microsoft-365/blog/2025/03/25/introducing-researcher-and-analyst-in-microsoft-365-copilot/</a></li><li><a href="https://marksmendaily.com/technology/openais-40b-fundraise-raises-agi-development-stakes/">https://marksmendaily.com/technology/openais-40b-fundraise-raises-agi-development-stakes/</a></li><li><a href="https://www.cnbc.com/2025/03/31/openai-closes-40-billion-in-funding-the-largest-private-fundraise-in-history-softbank-chatgpt.html">https://www.cnbc.com/2025/03/31/openai-closes-40-billion-in-funding-the-largest-private-fundraise-in-history-softbank-chatgpt.html</a></li></ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e1fc5500ea4a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Byte-Sized AI</title><link href="https://pedrotajia.com/blog/2025/byte-sized-ai/" rel="alternate" type="text/html" title="Byte-Sized AI"/><published>2025-03-31T22:08:30+00:00</published><updated>2025-03-31T22:08:30+00:00</updated><id>https://pedrotajia.com/blog/2025/byte-sized-ai</id><content type="html" xml:base="https://pedrotajia.com/blog/2025/byte-sized-ai/"><![CDATA[<p>March 2025</p> <p>By: Sahana Narayan</p> <h4>Welcome to Byte-Sized AI!</h4> <blockquote>Your bimonthly update on everything AI! Artificial intelligence continues to grow at an extraordinary pace, embedding itself into nearly every sector of today’s world. February’s developments underscore the rapid evolution of the field, highlighting both the emergence of new AI agents and the challenges faced by existing ones.</blockquote> <h4>1. Deepseek’s Embrace of Open-Source Principles</h4> <p>At the end of February, DeepSeek announced plans to share five of its repositories in the following week, emphasizing the company’s commitment to open-source principles and transparency.</p> <p>The Chinese artificial intelligence startup made waves in January with the release of its latest AI model, drawing comparisons to OpenAI’s ChatGPT. Many claimed that DeepSeek’s model not only rivaled OpenAI’s technology but surpassed it in efficiency, using less memory while maintaining high performance. According to the BBC, “That combination of performance and lower cost helped DeepSeek’s AI assistant become the most-downloaded free app on Apple’s App Store when it was released in the US.” However, its arrival also caused a dramatic drop in AI-related stocks, raising concerns among US companies.</p> <p>Amidst the buzz, allegations about data privacy quickly surfaced. Some cybersecurity experts claimed the models contained hidden codes that could potentially send private user data to the Chinese government, according to a report by GMA. This ignited widespread debate, with discussions suggesting a potential US government ban on the model. Ultimately, DeepSeek’s decision to open-source its repositories appears to be a strategic move toward transparency, possibly to deflect security concerns while reaffirming its dedication to openness.</p> <h4>2. OpenAI Introduces Operator Agent Around the World</h4> <p>In February, OpenAI launched its Operator agent in several countries–Australia, Brazil, Canada, India, Japan, and more–expanding the capabilities of AI-driven automation. OpenAI, famously known for Chat-GPT, explains that the agent is “capable of doing work for you independently — you give it a task and it will execute it.” For instance, it can be instructed to perform tasks like booking tickets, make restaurant reservations, or shop on websites. The agent operates using a technology called “Computer Using Agent (CUA)” which interprets screenshots similar to how a human would. This then allows it to “navigate graphical user interfaces (GUIs), such as buttons, menus, and text fields, enabling it to perform tasks.”</p> <p>There already exists ample competition in this landscape of AI agents, and some of these alternatives are free as well. WebUI (open-source browser agent framework), Stagehand (web automation framework), and Anthropic’s Claude (with focus on safety) are notable examples. Furthermore, early feedback on Operator has been mixed. Many users complained that Operator requires constant user oversight and the system is slow. However, the international rollout signals OpenAI’s commitment to refining the agent and creating a more effective tool in the future.</p> <h4>3. OpenAI legal battles in India and other countries</h4> <p>While OpenAI’s models have gained widespread popularity, the company has been facing mounting legal challenges. On February 13, 2025, India’s biggest music labels — including T-Series, Saregama, and Sony Music — announced that they were suing OpenAI for “unauthorized use of sound recordings” in training their AI models. The labels argued in a New Delhi court that OpenAI breached copyright laws by not obtaining the necessary licenses.</p> <p>This case follows a lawsuit filed by Asian News International (ANI), a leading Indian news agency that also claimed their content was used without permission by OpenAI. These legal battles extend beyond India; Germany’s music licensing company sued OpenAI for the same type of infringement in November of 2024. The New York Times has also sued the artificial intelligence company; in their case, they state, “millions of copyrighted works from the news organizations, articles that the publications argue were used without consent or payment — something the publishers say amounts to copyright infringement on a massive scale.” These ongoing cases will set a precedent for AI and copyright laws, with the outcomes potentially reshaping the way AI companies source training data.</p> <h4>4. Elon Musk debuts Grok 3</h4> <p>Towards the end of February, Elon Musk debuted Grok 3, the most advanced model yet of the AI assistant. Known for its real-time information sourcing from the X platform and its infamous “anti-woke” persona, Grok has carved out a unique niche in the AI landscape.</p> <p>Grok 3 unveils three core features: Think Mode, Big Brain Mode, and DeepSearch. Think Mode’s main purpose is multi-step reasoning, walking you through each step of its reasoning process; BigBrain Mode is utilized for solving highly complex problems. DeepSearch builds on Grok’s original use of X’s data, reasoning through “conflicting to provide concise, well-cited summaries. Unlike traditional search engines that dump raw links, DeepSearch processes and interprets.”</p> <p>Musk claims that this most recent model is “a maximally truth seeking AI, even if that means it is sometimes politically incorrect.” Whether this philosophy will win over users or spark further controversy remains to be seen, but Grok’s latest advancements place it among the most popular AI assistants on the market.</p> <h4>Sources:</h4> <p><a href="https://www.bbc.com/news/articles/c5yv5976z9po">https://www.bbc.com/news/articles/c5yv5976z9po</a></p> <p><a href="https://openai.com/index/introducing-operator/">https://openai.com/index/introducing-operator/</a></p> <p><a href="https://www.npr.org/2025/01/14/nx-s1-5258952/new-york-times-openai-microsoft">https://www.npr.org/2025/01/14/nx-s1-5258952/new-york-times-openai-microsoft</a></p> <p><a href="https://writesonic.com/blog/what-is-grok-3">https://writesonic.com/blog/what-is-grok-3</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f5fdf838d5f8" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">AI Plagiarism Detection</title><link href="https://pedrotajia.com/blog/2025/ai-plagiarism-detection/" rel="alternate" type="text/html" title="AI Plagiarism Detection"/><published>2025-03-05T01:46:43+00:00</published><updated>2025-03-05T01:46:43+00:00</updated><id>https://pedrotajia.com/blog/2025/ai-plagiarism-detection</id><content type="html" xml:base="https://pedrotajia.com/blog/2025/ai-plagiarism-detection/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fE0TNrEdUbBkwwX-"/><figcaption>Image From Turnitin</figcaption></figure> <p>By: Chloe Poon</p> <p>Additional Contributions: Sahana Narayan</p> <p>In Wichita, Texas, Midwestern State University student Jessica Zimny was assigned to write a short post for her political science class. She turned her post in, which was then scanned by an AI detector. The scan concluded that the post was sixty-seven percent written by artificial intelligence, and she received a zero. However, the scan was wrong. Zimny had not used AI.</p> <p>Enter AI writing detectors. With the rise of large language models like ChatGPT producing human-like writing, the education system has had to learn to fight back. You may have heard of Turnitin, a widely used platform by schools to detect AI writing. While Turnitin and other AI detectors have been proven to catch cheating in academia, they have a surprising rate of false positives.</p> <p>A “false positive” means that a fully human-generated text was identified as AI-generated. On their website, <a href="https://www.turnitin.com/blog/understanding-false-positives-within-our-ai-writing-detection-capabilities">Turnitin</a> maintains that its detectors do not “make a determination of misconduct.” In other words, a more accurate implication of a false positive would be an indication of generic writing, rather than evidence of AI use.</p> <p><a href="https://www.bloomberg.com/news/features/2024-10-18/do-ai-detectors-work-students-face-false-cheating-accusations"><em>Bloomberg Businessweek</em> performed a test</a> with 500 random Texas A&amp;M authentic college essay submissions written before ChatGPT was released (Davalos). Through two popular AI detection systems, 1% to 2% of essays were falsely identified as AI-written. Some papers were flagged as 100% written by AI while others only had sections flagged. Now, is this a random occurrence? Not quite.</p> <p>When <a href="https://www.washingtonpost.com/technology/2023/04/01/chatgpt-cheating-detection-turnitin/">the <em>Washington Post</em> spoke to Turnitin’s Vice President Eric Wang</a>, Wang claimed that Turnitin detects AI in writing that is “too consistently average” (Fowler). Large language models like ChatGPT create sentences using real human writing as examples. If you type on an iPhone, you probably see a bar that auto-completes your writing. Seen below in blue.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*DW0SeQNVepCGVTHW"/></figure> <p>It boils down to statistics in the question: “After this word, what is most likely the next word in the sentence?” In this context, the result would be labeled “generic writing” since it is based on a generic sample of word combinations.</p> <p>Ultimately, “AI writing is the most probable subset of human writing,” says Wang. This is what the AI detector looks for.</p> <p>Moreover, AI detectors can prove difficult for non-native English speakers. A <a href="https://arxiv.org/pdf/2304.02819">2023 study done by Stanford University</a> focused on measuring bias against non-native speakers (Liang). AI detectors were twice as likely to be critical of a non-native speaker’s essay versus a native speaker’s. This bias does not only apply to non-native English speakers. Many on the neurodivergent spectrum tend to be also biased against by their writing. This bias has caused blind spots in AI detection to bloom.</p> <p>As a result, many AI detection systems like to identify themselves as more “invitations” for criticism in student writing. However, many schools still treat AI detection as fact, and the unfortunate reality is that AI accusations can lead to much worse punishments. Suspension, expulsion, and other penalties on a record can significantly reduce students’ academic and professional opportunities.</p> <p>Students, like MSU student Jessica Zimny, have been looking for ways to combat this after being accused of using AI (Verma). Despite pleading her case to the department head and even the school dean, Zimny could not prove her innocence. When asked by the <em>Washington Post</em>, Zimny revealed that she resorted to <a href="https://www.washingtonpost.com/technology/2023/08/13/ai-chatgpt-chatbots-college-cheating/">screen-recording</a> herself writing assignments to prepare for the worst case.</p> <p>This dilemma highlights how AI detectors are treated as judges across schools rather than feedback machines. And unfortunately, that leaves students like Jessica Zimny still stuck trying to defend their own words.</p> <p><strong>References</strong></p> <p>Davalos, Jackie, and Leon Yin. “Do AI Detectors Work? Students Face False Cheating Accusations.” <em>Bloomberg.Com</em>, 18 Oct. 2024, <a href="http://www.bloomberg.com/news/features/2024-10-18/do-ai-detectors-work-students-face-false-cheating-accusations.">www.bloomberg.com/news/features/2024-10-18/do-ai-detectors-work-students-face-false-cheating-accusations.</a></p> <p>Fowler, Geoffrey. “We Tested a New ChatGPT-Detector for Teachers. It Flagged an Innocent Student.” <em>The Washington Post</em>, 3 Apr. 2023, <a href="http://www.washingtonpost.com/technology/2023/04/01/chatgpt-cheating-detection-turnitin/.">www.washingtonpost.com/technology/2023/04/01/chatgpt-cheating-detection-turnitin/.</a></p> <p>Liang, Weixin, et al. “GPT detectors are biased against non-native English writers.” <em>Patterns</em>, vol. 4, no. 7, July 2023, p. 100779, <a href="https://doi.org/10.1016/j.patter.2023.100779.">https://doi.org/10.1016/j.patter.2023.100779.</a></p> <p>Verma, Pranshu. “Professors Have a Summer Assignment: Prevent ChatGPT Chaos in the Fall.” <em>The Washington Post</em>, 13 Aug. 2023, <a href="http://www.washingtonpost.com/technology/2023/08/13/ai-chatgpt-chatbots-college-cheating/.">www.washingtonpost.com/technology/2023/08/13/ai-chatgpt-chatbots-college-cheating/.</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fc7f50c41c8c" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">The Black Box Problem</title><link href="https://pedrotajia.com/blog/2025/the-black-box-problem/" rel="alternate" type="text/html" title="The Black Box Problem"/><published>2025-02-23T01:39:38+00:00</published><updated>2025-02-23T01:39:38+00:00</updated><id>https://pedrotajia.com/blog/2025/the-black-box-problem</id><content type="html" xml:base="https://pedrotajia.com/blog/2025/the-black-box-problem/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*l3MEAQWBuOKfEo4O"/><figcaption>Image from UM-Dearborn News</figcaption></figure> <p>By: Elizabeth Louie</p> <p>Additional Contributions: Sahana Narayan, Pearl Vishen</p> <h3>Introduction</h3> <p>As artificial intelligence continues to rapidly develop, it is becoming more and more integrated into important aspects of our daily lives. For example, AI is now widely used in the medical field to identify patterns in medical images, therefore detecting certain diseases and enabling early diagnosis (Blouin). However, it is important for humans to trust the outputs machine learning systems provide and Explainable AI (XAI) can help to do so.</p> <h3>The Black Box Problem</h3> <p>As AI constantly advances, it has become harder to understand and retrace how an algorithm creates its output. This inability to see how deep learning systems calculate processes of algorithms is called the “black box” problem. The same way humans have implicit knowledge, deep learning algorithms lose track of inputs that were used during algorithm training (Blouin). As AI has developed great accuracy by employing more complex algorithms, these contemporary systems can’t explain their decisions straightforwardly.</p> <p>Now that AI is being used in the medical field, for mental health resources, and for education, the “black box” problem can become a serious ethical issue. For instance, if an autonomous car hits a pedestrian, humans are unable to trace back to where the AI system failure occurred (Blouin). Explainable AI (XAI) focuses on ensuring a transparent understanding when utilizing AI systems, by clearly describing an AI model’s decisions and its expected impact while meeting regulatory standards.</p> <h3>Explainability Techniques</h3> <p>XAI is based on the concept of explainability. ScienceDirect, an online database of peer-reviewed articles, defines explainability as, “the process of elucidating or revealing the decision-making mechanisms of models.” There are a variety of techniques for explainability, providing insights to machine learning decisions.</p> <p>One is called a scoop-based explainer and uses feature importance analysis, a technique that calculates how much impact each input has on a machine learning model’s prediction of a target variable (Ali et. al). The analysis is categorized as local or global. The local method is limited to a single decision or instance with only one explanation whereas the global method provides rationale for the entire data set.</p> <p>A good example of a local method is LIME, which stands for local interpretable model-agnostic explanations. LIME alters original data points, feeds these back into the black box, and observes the output in response to the modified data (Dhinakaran). This strategy assigns weight to new data points as in some will matter more than others, determined by how close new data points are to the original. Ultimately, this creates a simpler model approximating the behavior of the original more complex model. The goal of LIME is to identify an interpretable model, something easier for humans to understand such as a binary tree where decision making is clear.</p> <h3>A Four Axes Model for Explainability</h3> <p>A research article published on ScienceDirect by Sajid Ali and other researchers, proposes a four axes methodology for explainability in deep neural networks. This framework analyzes and evaluates XAI by considering four distinct dimensions to allow for a multifaceted examination. In other words, this model looks at XAI from four different angles using a hierarchical categorization system to gain a comprehensive understanding. SceinceDirect proposes this model to “diagnose the training process and to refine the model for robustness and trustworthiness.” Each axis has various research questions to guide inquiry, as well as a taxonomy to classify categorization of concepts associated with each axis. All four axes are important for an adequate understanding of the explanation.</p> <p>The first axis is data explainability, which uses tools and various methods to summarize and analyze data to offer subsequent understanding of the data used to train AI models. This axis is important because the performance of an AI model is influenced by the characteristics of the data it’s trained on. Aspects of data explainability include comprehending knowledge graphs, data summarizing, exploratory data analysis, and any preprocessing or transformations applied (Ali et. al). Some of the research questions that ScienceDirect proposes to address this axis of explanation include: What sort of information do we have in the database? What can be inferred from this data? What are the most important portions of the data? (Ali et. al) These questions explore the dataset’s content, relevance, usability, and the axis’s role in improving model interpretability and performance. Data explainability offers insights to how open and understandable the AI model is to users, while focusing on the inner workings such as its decision-making processes providing overall transparency within the four axes model.</p> <p>The second axis is model explainability, which shows the internal structure and algorithms of an AI model, creating an understanding on how the model processes inputs to produce outputs. Model explainability focuses on interpretability (Ali et. al). ScienceDirect defines interpretability by saying it “enables developers to delve into the models decision making process, boosting confidence in understanding where the model gets its results.” This can involve selecting model types that are easier to interpret such as linear regression or decision trees as used in LIME. The importance of the model explainability axis is that it makes AI systems interpretable for humans. An example of this for a neural network might involve techniques that visualize which layers are responsible for certain types of information. Research questions guiding this axis include: What makes a parameter, objective, or action important to the system? When did the system examine a parameter, objective, or action and when did the model reject it? What are the consequences of making a difference decision or adjusting a parameter? (Ali et. al). These prompts look at how the AI model operates and what factors affect the model’s behavior.</p> <p>The next axis is post-hoc explainability, designed to elucidate significant features of an AI model using several kinds of explanation. Post-hoc explainability as described by ScienceDirect “refers to methods/algorithms that are used to explain AI model’s decisions.” The research questions relevant to evaluation post-hoc explainability include: What is the reason behind the model’s prediction? What was the reason for occurrence X? What variables have the most influence on the user’s decision? (Ali et. al). The overall purpose of this axis is to allow users to understand individual predictions without requiring full transparency into the model’s internals by interpreting the decision making process.</p> <p>The last axis, assessment of explanations, ensures that explanations are clear, accurate, and useful for different audiences. The criteria for assessment includes completeness, fidelity, and comprehensibility. The purpose of this last axis is to ensure meaningful and technically accurate explanations for XAI users. Altogether these axes support a robust approach to creating trustworthy AI systems.</p> <h3>Conclusion</h3> <p>Explainable AI helps us characterize model accuracy and transparency in areas that have previously been uninterpretable. As AI continues to develop in various areas in our lives ranging from search engine optimization to applications in the medical fields, advocating for transparency is crucial. Deep learning systems give rise to the black box problem when more sophisticated algorithms are employed, and the complex nature of these algorithms make it so these systems are unable to explain its decisions in a straightforward manner. This issue makes it even harder to trust the outputs of AI that we constantly rely on.</p> <p>Explainable AI provides a framework of transparency for AI users. As mentioned, there are many approaches to XAI techniques. A scoop based explainer categorizes feature importance analysis into either local or global analysis. Local methods are limited to a single explanation whereas the global method explains the entire data set. The previous example was LIME, which approximates the behavior of complex systems in order to provide an explanation. Researchers from ScienceDirect propose another technique for XAI involving a four axes framework to provide layered explanations of AI models. Using these various methods of explainable AI, we can put more faith into these complex algorithms and deep learning systems that are becoming increasingly prevalent in our society.</p> <h3><strong>References</strong></h3> <p>Ali, Sajid, et al. “Explainable Artificial Intelligence (XAI): What We Know and What Is Left to Attain Trustworthy Artificial Intelligence.” Information Fusion, Elsevier, 18 Apr. 2023, <a href="http://www.sciencedirect.com/science/article/pii/S1566253523001148.">www.sciencedirect.com/science/article/pii/S1566253523001148.</a></p> <p>Blouin, Lou. “Ai’s Mysterious ‘black Box’ Problem, Explained.” Dearborn, umdearborn.edu/news/ais-mysterious-black-box-problem-explained. Accessed 26 Jan. 2025.</p> <p>Dhinakaran, Aparna. “What Are the Prevailing Explainability Methods?” Medium, Towards Data Science, 22 Dec. 2021, towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c40d3c6f26fe" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Male is to Architect, Female is to Interior Decorator: Word Embedding Gender Bias</title><link href="https://pedrotajia.com/blog/2025/male-is-to-architect-female-is-to-interior-decorator-word-embedding-gender-bias/" rel="alternate" type="text/html" title="Male is to Architect, Female is to Interior Decorator: Word Embedding Gender Bias"/><published>2025-02-09T21:06:38+00:00</published><updated>2025-02-09T21:06:38+00:00</updated><id>https://pedrotajia.com/blog/2025/male-is-to-architect-female-is-to-interior-decorator-word-embedding-gender-bias</id><content type="html" xml:base="https://pedrotajia.com/blog/2025/male-is-to-architect-female-is-to-interior-decorator-word-embedding-gender-bias/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fw3MDu43-Iwgnm5NFmWHyQ.jpeg"/></figure> <p>By: Rakshita Narayanaswamy</p> <p>Additional Contributions: Sahana Narayan, Pearl Vishen</p> <p>Word embeddings are a key concept in natural language processing (NLP), a field within machine learning that deals with understanding and generating human language. The communication tool Google Translate is a widely recognized example, leveraging these techniques to analyze the structure and meaning of sentences to provide translations. Let’s run an example of this:</p> <p>Take the sentence: “She is an architect. He is an interior decorator.”</p> <p>Let’s first translate it into Armenian.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*zqxn7ipXhmp1abnW"/></figure> <p>Now, let’s translate this back to English:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*63prcOwWgSRQi7TP"/></figure> <p>If we translate this sentence into a non-gendered language (Armenian) and then back to a gendered one (English), we might see the professions switch, with “He” becoming the architect and “She” the interior decorator. This demonstrates how word embeddings are utilized in a tool like Google Translate to conceptualize language and, unintentionally, propagate societal biases.</p> <h3><strong>What are Word Embeddings?</strong></h3> <p>Most machine-learning models are incapable of interpreting plain text in its raw form. Thus, word embeddings are treated like dictionaries for machines, having an algorithm translate a word into its numerical representation. In Natural Language Processing, words are represented as vectors in a high-dimensional space, forming a complex network of associations. Imagine this space as a maze of connections, where each path between words represents a semantic relationship. Words with similar meanings or contextual usage–like child and infant–are positioned closer together, while unrelated words are further apart.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*vG0cRJ6qW_7CIIh4"/><figcaption>Figure 1: Semantic Feature subspace comparing age to gender [3]</figcaption></figure> <p>Through capturing the hidden landscapes within language, certain paths (like the professions “architect” and “interior decorator”) become linked with gendered ‘directions’, demonstrating the inherent biases within the data it’s trained on.</p> <p>Modern models are trained on a copious amount of text data — typically made up of journalistic, social media, or other culturally sourced texts. As a result, these language models operate similarly to a ‘mirror’, capturing and amplifying the cultural attitudes and stereotypes embedded within society. When NLP models use these embeddings for tasks like translation, these biases can surface, as we saw in the example above. Google Translate relies on contextual word embeddings that analyze entire sentences to capture word relationships within the text to generate the embedding. As such, when asked to re-assign pronouns and fill contextual gaps, the model relied on the associations it learned to predict the most statistically probable option-making gender-based choices rather than neutral ones.</p> <h3><strong>Data: Closer Inspection of Professional Bias</strong></h3> <p>Profession bias is one of the most evident forms of gender disparities. In a 2016 study at Cornell University, Bolukbasi and his team were one of the first to identify this problematic relationship. The researchers explored simple analogies and comparisons regarding occupations or gender roles, and heavy stereotypes surfaced: for instance, “man” is closer to “doctor,” and “woman” is closer to “nurse” — or “computer programmer” is to “man” as “homemaker” is to “woman” [1].</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/828/0*-QtMk4Jzer0l4RwY"/><figcaption>Figure 1: Examining occupations heavily associated with embeddings of the words <em>he-she, </em>based on training from g2vNEWS text [1].</figcaption></figure> <p>In 2022, a study led by Caliskan at the University of Washington explored the gender biases within word embeddings, focusing on how models group words and concepts in relation to their meanings in the human language. Researchers conducted cluster analysis on the top words associated with gender and found a notable difference between socially “male” and “female” gendered topics. Top “male” concepts surrounded topics of leadership, technology, and physical strength–“engineering,” “CEO,” “sports.” Contrastingly, “female” concepts were less focused on positions and included terms like “beautiful,” “marriage,” “spa,” alongside more troubling associations with derogatory slurs and sexualized language [2].</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Yc3Z1MJ1scrdjnF3"/><figcaption>Table 3: Top associated concepts of 1,000 female and 1,000 male associated words [2].</figcaption></figure> <p>These clusters reflected not only professional disparities but also broader societal biases that prioritize men in positions of power, reducing women to domestic or objectified roles. For example, although a word like beautiful is subjectively positive, it is discriminatory if it reduces women’s associations with other words, such as professional.</p> <h3><strong>Societal Implications</strong></h3> <p>Associating certain professions and activities with specific genders causes representational harm by reinforcing stereotypes about what men and women are capable of achieving. When these biased associations are embedded in widely used language models, they do more than reflect societal attitudes — they actively shape them, leading women and girls to internalize these stereotypes and narrow the scope of what they aspire to achieve. As AI systems become more pervasive, they risk perpetuating these biases, projecting limiting views onto future generations and further strengthening gender inequality in society.</p> <h3><strong>De-Biasing The Systems</strong></h3> <p>Debiasing these systems is a process that has been long sought after. The end goal is to remove the underlying prejudices these gender-neutral vectors possess while maintaining their semantic relationships. As a result, researchers have invented many mitigation techniques, with post-processing techniques being the most notable. Subspace is a specific method of post-processing that focuses on identifying the “bias direction” in the embedding space, analyzing the relationships between words known to exude bias–”men” , “women”, “he”, “she”. This technique aims to remove the gendered directionality from these word relationships, making them neutral [1]. In most cases, even if metrics showcase an adjustment to the bias, it is still present and recoverable. This underscores the critical need for further research in this area to foster ethical AI practices that do not discriminate against any groups.​ By advancing our understanding and methods to effectively eliminate bias, we can pave the way for more equitable and just artificial intelligence systems.</p> <p><strong>References</strong></p> <p>[1] Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings, Bolukbasi et al., 2016, <a href="https://arxiv.org/pdf/1607.06520.pdf">https://arxiv.org/pdf/1607.06520.pdf</a></p> <p>[2]Semantics derived automatically from language corpora contain human-like biases, Caliskan et al., 2017, <a href="https://arxiv.org/pdf/2206.03390v1">https://arxiv.org/pdf/2206.03390v1</a></p> <p>[3]Semantic Feature Space, <a href="https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/tutorial.html">https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/tutorial.html</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=01b775062413" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Training Generative AI: Is it Copyright Infringement?</title><link href="https://pedrotajia.com/blog/2025/training-generative-ai-is-it-copyright-infringement/" rel="alternate" type="text/html" title="Training Generative AI: Is it Copyright Infringement?"/><published>2025-02-05T01:54:44+00:00</published><updated>2025-02-05T01:54:44+00:00</updated><id>https://pedrotajia.com/blog/2025/training-generative-ai-is-it-copyright-infringement</id><content type="html" xml:base="https://pedrotajia.com/blog/2025/training-generative-ai-is-it-copyright-infringement/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/797/0*fnjkY1gnqcRl9ICG"/><figcaption>Image by Marianas Variety</figcaption></figure> <p>By: Alinda Lau</p> <p>Additional Contributions: Sahana Narayan, Pearl Vishen</p> <p>Many questions on copyright infringement have been emerging as usage of generative AI has become increasingly widespread. However, one question has been particularly recurrent: should companies be allowed to train their AI models on other people’s work without authorization?</p> <p>After being trained on large amounts of data, generative AI becomes capable of creating imitations of the content it was trained on. If a model is trained with large amounts of textual content, such as academic papers and journal articles, then it will generate output text that utilizes patterns from the data it was trained on.</p> <p>Currently, the most well-known generative AI model is OpenAI’s ChatGPT. Given the model’s popularity, OpenAI has become a primary target of copyright infringement lawsuits.</p> <p>In December of 2023, The New York Times filed a lawsuit against Microsoft and OpenAI, accusing them of violating copyright with their “unlawful use of The Times’s work to create artificial intelligence products that compete with it.” After being trained on datasets that included NYT articles, the defendants’ models were able to generate outputs that were highly similar, if not completely identical, to The Times’ original content. In response to NYTimes’ allegations, OpenAI argued that “[t]raining AI models using publicly available internet materials is fair use.”</p> <p>The U.S. Copyright Office defines fair use as “a legal doctrine that promotes freedom of expression by permitting the unlicensed use of copyright-protected works in certain circumstances.” Section 107 of the Copyright Act, an act that protects original works of authorship, provides the following 4 factors to evaluate whether something falls under fair use or not:</p> <ol><li>“The purpose and character of the use, including whether such use is of a commercial nature or is for nonprofit educational purposes.”</li><li>“The nature of the copyrighted work” — whether the copyrighted work reflects creative expression or factual reporting</li><li>“The amount and substantiality of the portion used in relation to the copyrighted work as a whole.” (In some circumstances, even using a small portion of a copyrighted work can be ruled as not fair use because it was an important part of the work’s essence.)</li><li>“The effect of the use upon the potential market for or value of the copyrighted work” — whether the instance of unlicensed use harms the market for the original work.</li></ol> <p>The fourth factor is particularly of concern for the Times as they are concerned that ChatGPT’s verbatim recitation of their content will make people drastically less likely to pay for a subscription to their publication.</p> <p>It is also worth noting that, under the first factor of consideration, unauthorized use of copyright-protected works is far more likely to be considered fair use if the new content uses the original copyright-protected content in a “transformative” way — in a way that adds something new to it.</p> <p>OpenAI leans into this, emphasizing that their main goal — which they claim to have successfully executed for the most part — is for generative AI to produce transformative outputs based on the data it was trained on. Despite The Times’ accusations, OpenAI says that verbatim recitation is “a rare failure of the learning process that [they] are continually making progress on,” not a common occurrence nor the intended use of their technology. They insist that their models are “designed and trained to learn concepts in order to apply them to new problems,” not to simply replicate existing content.</p> <p>With no end date in sight, it is apparent that this case is only the beginning of the clash between journalism and generative AI. Content creators of other specialities, authors and artists, have also filed fair use lawsuits against AI and tech companies. Though the continuous progression of AI technology is exciting, standards for regulation must progress alongside it, or we may find ourselves in a bottomless pit of compounding legal and ethical concerns.</p> <p>References</p> <p>[1] <a href="https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec2023.pdf">https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec2023.pdf</a></p> <p>[2] <a href="https://openai.com/index/openai-and-journalism/">https://openai.com/index/openai-and-journalism/</a></p> <p>[3] <a href="https://www.copyright.gov/fair-use/">https://www.copyright.gov/fair-use/</a></p> <p>[4] <a href="https://www.copyright.gov/title17/92chap1.html#107">https://www.copyright.gov/title17/92chap1.html#107</a></p> <p>[5] <a href="https://openai.com/index/openai-and-journalism/">https://openai.com/index/openai-and-journalism/</a></p> <p>[6] <a href="https://openai.com/index/openai-and-journalism/">https://openai.com/index/openai-and-journalism/</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0ea5c2ad46f7" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Natural Language Processing Simplified</title><link href="https://pedrotajia.com/blog/2025/natural-language-processing-simplified/" rel="alternate" type="text/html" title="Natural Language Processing Simplified"/><published>2025-02-03T08:15:32+00:00</published><updated>2025-02-03T08:15:32+00:00</updated><id>https://pedrotajia.com/blog/2025/natural-language-processing-simplified</id><content type="html" xml:base="https://pedrotajia.com/blog/2025/natural-language-processing-simplified/"><![CDATA[<p>By: Riley Menter &amp; Syed Islam</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RlOS5r_Qta_9lu8J2KEypQ.jpeg"/></figure> <p><strong>Introduction</strong></p> <p>In recent years, artificial intelligence (AI) has profoundly impacted society through its applications in industry, education, and everyday tasks. You’ve likely heard the term “AI” before, but do you know what it entails? Some might say machine learning, deep learning, or even statistics — and while these are all key components of AI, one critical aspect often flies under the radar: Natural Language Processing, or NLP for short. So, what exactly is NLP? In a nutshell, it’s the subfield of AI focused on enabling computers to understand, interpret, and generate human language in meaningful ways. NLP acts as a bridge between machines and the way we communicate, allowing them to make sense of our words and respond accordingly.</p> <p>A prime example of NLP in action is ChatGPT, the AI language model that’s taken the world by storm. When you type a prompt into ChatGPT, NLP algorithms work behind the scenes to decipher your input, grasp its context and intent, and formulate a relevant, coherent response. The advanced level of language understanding allows ChatGPT to engage in such impressive dialogue — and it’s just one illustration of NLP’s immense potential. From virtual assistants like Siri and Alexa to automated translation services, spam filters, and beyond, NLP is like the secret formula that makes AI feel more human. By giving machines the power to comprehend and communicate in our native tongue, NLP opens up a world of possibilities for more intuitive, natural interactions between humans and computers. As AI continues to embed itself in every corner of our lives, it’s clear that NLP will only become more essential in bridging the gap between man and machine.</p> <p>In this article, we’ll dive deep into the fascinating field of Natural Language Processing, exploring its core concepts, real-world applications, and future trajectories. Whether you’re an AI enthusiast or simply curious about this amazing technology, this is your chance to learn about NLP and discover how it’s shaping our relationship with artificial intelligence.</p> <p><strong>What Does NLP Involve?</strong></p> <p>Now that we have a general understanding of NLP, let’s explore its fundamental components and how they enable computers to process human language. At its core, NLP is all about teaching computers to process and understand the complexities of human language. And to do that, we start with the fundamental building blocks of language itself. Think of it like constructing a house. Just as a home is made of individual bricks, beams, and tiles, language is composed of basic units called tokens — which are smaller parts of a text including words, characters, or sentences. Tokenization is the process of breaking down text into these tokens. It’s the first step in helping computers interpret a piece of writing.</p> <p>But language is more than just a bag of words. The way those words are strung together, the roles they play in a sentence — these factors impact its meaning. That’s where techniques like part-of-speech tagging and parsing come in. Imagine diagramming a sentence like you did in grade school. You identify the nouns, verbs, adjectives, and so on (that’s part of speech tagging) and map out how they relate to each other (that’s parsing). NLP algorithms essentially do the same thing, giving computers a structural understanding of language. Now here’s where things get interesting. To take this linguistic knowledge and use it to comprehend language in a humanlike way, computers lean on two powerful tools: statistics and artificial intelligence.</p> <p>In the early days of NLP, statistical methods reigned supreme. These approaches boil down to cleverly counting words and phrases and looking for patterns and correlations. While these techniques are still used today, the emergence of AI has revolutionized NLP’s potential. The secret weapon? Neural networks. These brain-inspired algorithms can learn from massive amounts of human-generated text and discover deeply buried patterns and relationships. The result is language models — AI systems that can predict, interpret, and generate human language with remarkable fluency.</p> <p>If you’ve ever stood in awe at the comprehensibility of a chatbot or the accuracy of a voice assistant, you can thank neural language models. Under the hood, they’re leveraging the linguistic foundation of NLP and the predictive power of neural networks to engage in impressive human-like communication. Of course, this is a simplified view. The reality of NLP involves sprawling neural architectures with billions of parameters, trained on terabytes of text data. But at a conceptual level, it all comes back to those core ideas — breaking language down into its basic components, analyzing the structure and relationships, and harnessing the pattern-recognition might of AI. As we continue our journey into the world of NLP, keep these building blocks in mind. They form the bedrock upon which all the flashy applications and transformative use cases are built. Understanding how they snap together is key to explaining NLP — and imagining where this revolutionary technology might take us next.</p> <p><strong>History of NLP</strong></p> <p>The field of Natural Language Processing (NLP) has a rich history spanning several decades. Let’s take a quick tour through some of the key eras that shaped NLP into the powerful technology it is today.</p> <p>The 1950s marked the birth of NLP, with early attempts at machine translation and the emergence of rule-based systems. These systems relied on hand-crafted linguistic rules to analyze and generate language. While groundbreaking for their time, rule-based approaches struggled to capture the full complexity and nuance of human language.</p> <p>Fast-forward to the 1980s and 1990s, and we see a major shift toward statistical methods and machine learning. Instead of relying solely on predefined rules, these approaches leveraged large amounts of language data to learn patterns and make predictions. This transition unlocked new possibilities for NLP, enabling more robust and flexible language processing.</p> <p>The 2010s ushered in the deep learning revolution, which completely transformed NLP. The introduction of word embeddings allowed words to be represented as dense vectors, capturing semantic relationships in a way that was previously impossible. Neural networks, particularly architectures like recurrent neural networks (RNNs) and transformers, enabled NLP models to learn and generalize from vast amounts of text data.</p> <p>In recent years, the rise of large language models (LLMs) like BERT and GPT-3 has pushed the boundaries of what’s possible with NLP. These models, pre-trained on massive corpora (documents they are trained on) of text, can perform a wide range of language tasks with remarkable accuracy and fluency. They’ve unlocked new possibilities for more natural, human-like communication between people and computers, bringing us closer to a future where talking to AI feels as easy and familiar as chatting with a friend.</p> <p>From its humble beginnings in the 1950s to today’s cutting-edge advancements, NLP has undergone a remarkable evolution. Each era brought new techniques and insights that built upon the foundations laid by previous generations of researchers and practitioners. As we look to the future, it’s clear that NLP will continue to shape the way we interact with technology and unlock new possibilities for communication and understanding.</p> <p><strong>Real World Applications</strong></p> <p>NLP is making significant waves in a variety of real-world applications, transforming how we interact with technology and extract insights from vast amounts of unstructured text data. One of the most familiar examples is virtual assistants and chatbots. By leveraging NLP, these AI-powered tools can understand and respond to user queries in natural, human-like conversations. As NLP continues to advance, we can expect these interactions to become even more seamless and intuitive, greatly enhancing customer support and engagement across industries.</p> <p>Another critical application of NLP is sentiment analysis. In today’s digital landscape, businesses must monitor customer sentiment to stay competitive. NLP algorithms can analyze the emotional tone behind social media posts, online reviews, and other text data sources, helping companies gauge brand perception, address customer concerns, and identify growth opportunities in real time. This capability is essential for organizations looking to respond quickly to customer needs and maintain a positive online presence.</p> <p>NLP is also revolutionizing document processing, particularly in document-heavy sectors such as legal, healthcare, and finance. By automating tasks like document categorization, information extraction, and summarization, NLP streamlines workflows and reduces manual effort. These capabilities allow professionals to focus on higher-value tasks while ensuring that important information is readily accessible and actionable.</p> <p>In healthcare, NLP holds immense potential for improving patient outcomes and reducing costs. By extracting insights from unstructured medical data, such as clinical notes and patient records, NLP can inform treatment decisions, identify at-risk patients, and support personalized medicine initiatives. As healthcare organizations grapple with the challenges of managing vast amounts of data, NLP will play a crucial role in driving data-driven improvements in care delivery.</p> <p>Finally, NLP is becoming an indispensable tool for monitoring social media and managing reputation. By tracking sentiment, influencers, competitors, and emerging trends across social channels, organizations can gain valuable insights into their online presence and more effectively engage their target audiences. NLP-powered social listening platforms can also help companies detect and mitigate potential crises before they escalate, protecting their brand reputation in an increasingly complex digital environment.</p> <p>As NLP technology continues to evolve at a rapid pace, its impact on our lives will only grow. The ability to extract actionable intelligence from the vast amounts of unstructured text data generated every day will transform industries, improve decision-making, and unlock new possibilities for innovation. While the NLP revolution is already well underway, its most significant and transformative impacts are yet to come. As businesses and individuals alike seek to harness the power of language data, those who stay at the forefront of NLP will be well-positioned to reap the benefits of this exciting and dynamic field.</p> <p><strong>Ethical Concerns and Biases</strong></p> <p>Although NLP has driven amazing advancements, it also raises significant ethical concerns that must be addressed. These concerns primarily revolve around bias, fairness, and privacy. As this powerful technology continues to advance, companies and organizations must prioritize ethical considerations.</p> <p>Bias and fairness are critical aspects of NLP. Bias can occur at various stages, including data collection, preprocessing, and algorithmic design. For example, let’s say that a model was built using datasets that lack diversity or fail to represent its target population. As a result, this could unintentionally reinforce societal stereotypes, potentially leading to discriminatory, inappropriate, or inaccurate outputs. However, it is also possible that the dataset itself isn’t the issue. Instead, bias can occur due to an algorithm’s feature and parameter designs. Addressing these biases requires a careful selection of datasets and algorithms to minimize unfairness. Evaluations should be conducted at both stages to avoid these issues. Doing so ensures that a model respects different backgrounds and groups fairly, regardless of demographic characteristics and social stereotypes.</p> <p>Privacy is another critical concern since NLP often builds models using sensitive data such as financial records, health information, or personal identifiers. Robust measures such as encryption and secure storage protect sensitive data. Another important aspect of privacy is to obtain informed consent from users before collecting and processing their data. This is often done through pop-ups in apps, sites, and software with an opt-out option that tells the user the purpose of collecting data. If consent is given, anonymizing data — by removing personally identifiable information — is the next step. This step safeguards user privacy by making it impossible to link the data to any person. These measures ensure that NLP systems respect individual rights.</p> <p>Lastly, transparency and accountability are also essential for ethical NLP development. Advanced NLP models often function as “black boxes”, meaning that it has complex architectures and decision-making processes that are hard to interpret, even for developers. While their complexity can yield high accuracy, it often comes at the cost of transparency, making it difficult to identify the sources of bias. As NLP Systems continue to advance in the fast-changing tech world, developers must focus on model interpretability as it allows organizations and users to understand how decisions are made. Organizations must also take responsibility for their system’s societal impact, promptly addressing issues that arise regarding data protection. Sharing information about data sources, training methods, and model architecture further promotes transparency. By prioritizing these principles, NLP technologies can advance ethically while enhancing society.</p> <p><strong>Key Takeaways on NLP</strong></p> <p>Natural Language Processing is a crucial component of artificial intelligence because it enables machines to understand human communication. Its fundamental features and evolution have transformed the way we interact with technology. However, its advancements come with ethical concerns, including bias, fairness, privacy, and transparency. NLP will continue to have a significant role in human-AI interactions, emphasizing the need for careful development and application. By addressing these concerns with safeguards, transparent models, and a commitment to upholding ethical development, NLP can continue to advance and improve society with its incredible technology.</p> <p><strong>Works Cited</strong></p> <ol><li>“Ethical Considerations in Natural Language Processing: Bias, Fairness, and Privacy.” <em>GeeksforGeeks</em>, GeeksforGeeks, 5 Dec. 2023, <a href="http://www.geeksforgeeks.org/ethical-considerations-in-natural-language-processing-bias-fairness-and-privacy/.">www.geeksforgeeks.org/ethical-considerations-in-natural-language-processing-bias-fairness-and-privacy/.</a></li><li>“History and Evolution of NLP.” GeeksforGeeks, GeeksforGeeks, 10 May 2024, <a href="http://www.geeksforgeeks.org/history-and-evolution-of-nlp/.">www.geeksforgeeks.org/history-and-evolution-of-nlp/.</a></li><li>“Natural Language Processing (NLP) — A Complete Guide.” <em>(NLP) [A Complete Guide]</em>, <a href="http://www.deeplearning.ai/resources/natural-language-processing/.">www.deeplearning.ai/resources/natural-language-processing/.</a></li><li>“Natural Language Processing (NLP) — Overview.” GeeksforGeeks, GeeksforGeeks, 6 Dec. 2024, <a href="http://www.geeksforgeeks.org/natural-language-processing-overview/.">www.geeksforgeeks.org/natural-language-processing-overview/.</a></li><li>Gruetzemacher, Ross. “The Power of Natural Language Processing.” Harvard Business Review, 6 Sept. 2024, hbr.org/2022/04/the-power-of-natural-language-processing.</li></ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=49a7fe6c4dff" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Self-Supervised Learning: What is it?</title><link href="https://pedrotajia.com/blog/2025/self-supervised-learning-what-is-it/" rel="alternate" type="text/html" title="Self-Supervised Learning: What is it?"/><published>2025-01-29T06:01:46+00:00</published><updated>2025-01-29T06:01:46+00:00</updated><id>https://pedrotajia.com/blog/2025/self-supervised-learning-what-is-it</id><content type="html" xml:base="https://pedrotajia.com/blog/2025/self-supervised-learning-what-is-it/"><![CDATA[<p>By: Pedro Tajia, Aanchal Acharya</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/825/0*KxFxMv9jvTViNM8c"/></figure> <p><strong><em>Figure 1:</em></strong><em> X is augmented to generate two kinds of examples v and v’ these new examples are imputed into the model f with parameters. The model outputs a vector that represents the input into a lower dimension. The model f is trained by using Self-Supervised learning to output a vector that contains rich representations.</em></p> <p>When working with artificial intelligence, the vast amount of data that is there brings with it many different types of data, as well. We must train models with data, and some training methods include supervised learning, unsupervised learning, reinforcement learning, and more. This article will discuss one specific branch of unsupervised learning, known as self-supervised learning. Self-supervised learning is a machine learning technique in which a model learns representations or features from unlabeled data by generating its own supervision signal. Another way to think about it is that it “fills in the blanks” of data on its own rather than humans having to do it manually. It allows training to take place efficiently on unlabeled data, which is very tedious and time consuming if done any other way.</p> <p><strong>What is it?</strong></p> <p>Self-supervised learning (SSL) is the process where the model is trained with unlabeled data by creating its own labels from the data itself, that is why it is called “self-supervised learning”. The use of supervised learning limits the ability of the model to understand the underlying patterns of the data, by labeling information the model is forced to learn the information in our way, instead of the model finding the meaning of the data by itself.</p> <p>Self-supervised learning is the phase where the model learns the structure and semantics of the data. Supervised Learning drives this initial knowledge, which is directed toward solving a specific task. Supervised learning forces the model to mold their knowledge into our perspective.</p> <p>Supervised learning is done by using label data to solve a task which serves as a guide to the pre-trained model from SSL toward learning a specific task, like predicting the name of a car based on images.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3xM-DnXBJahYrYwt"/></figure> <p><strong><em>Figure 2: </em></strong><em>In the section on self-supervised learning, the network f task is to learn the semantics or the meaning of the inputs for example the texture, contours, shading, etc. Then supervised learning to fine-tune the network on label datasets. In this case, learn to predict the class probabilities of the Model of a car by giving an image. In this case, labeled data is used to guide the network f to predict class probabilities. For example, given an image of a car, the network is trained to classify the model of the”car” by recalling the previously learned features with the labeled category.</em></p> <p>The success of LLMs like ChatGPT, Gemini, LLaMa, etc comes from Self-Supervised learning. These models are trained to predict missing words in a text, and then compare the prediction with the original text; this process is called <strong>Masked Language Modeling (MLM). </strong>In this setup, the model learns the syntax and semantics of the text to do a good job of filling the “correct” words.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*tjWBaC0HiHad1ilw"/></figure> <p><strong><em>Figure 3: </em></strong><em>This is a rough illustration of applying self-supervised learning in the training of a Large Language model (LLM). The first stage of training an LLM is using SSL to learn the syntax and semantics of words from a large corpus of text. In the training examples some words are masked or replaced, the task of the model is to reconstruct the text into its original state. When the model outputs diverge from the original text the error is propagated through the model to improve its predictions.</em></p> <p>There is a similar implementation on images where an image is corrupted and the model, like an autoencoder, tries to reconstruct the image to its original state; this is called <strong>denoising autoencoder.</strong></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*7X_FutOgSG0hnUM3"/></figure> <p><strong><em>Figure 4:</em></strong><em> The task of the denoising autoencoder is by giving a corrupted image the model needs to reconstruct the image into the original image. The error is calculated by finding the mismatch between the reconstructed image and the original image, the error is propagated on the model to improve its predictions.</em></p> <p>There are many ways of applying SSL, now in these days it has been developing more sophisticated frameworks to archive state-of-the-art results. For a deeper dive into these concepts, consider reading <a href="https://sthalles.github.io/simple-self-supervised-learning/">Simple Self-Supervised Learning</a> and <a href="https://pedrotajia.com/2024/10/20/bootstrap-your-own-latent.html">Bootstrap Your Own Latent</a>.</p> <p><strong>Current applications</strong></p> <p>Self-supervised learning is a technique that can be applied in any field that utilizes artificial intelligence. There are currently many applications for self-supervised learning, ranging from areas such as finance to healthcare.</p> <p>The use of self-supervised learning in computer vision has helped professionals in various fields enhance their performances. It has trained models to detect abnormalities in medical images such as X-rays and CT scans. Autonomous vehicles have also been consistently trained with images and videos of road signs, lanes, and other foreign objects to help them recognize patterns and drive in a safe manner.</p> <p>Self-supervised learning is very useful in natural language processing, as well. It is the basis of many language models, such as BERT and GPT. These models are trained using large amounts of text to complete tasks such as generating text, answering questions, and much more. It is also used for sentiment analysis when it comes to comments on social media, customer reviews on a product, public opinion analysis in politics, and much more. Chatbots and virtual assistants use self-supervised learning to generate responses to be as human-like as possible.</p> <p>Models are trained with many different types of data when it comes to processing speech and audio. Training the model using self-supervised learning allows it to process speech and audio with higher accuracy. It can detect vocabulary from the choice of words in speeches, such as lectures and interviews, and transcribe it to text. It also can pick up on sentiment from a person’s voice in audios and correctly identify sounds in recordings.</p> <p>In the finance industry, self-supervised learning has been used to help detect fraudulent activity in transaction data by identifying unusual patterns. It also helps with early risk detection to help prevent the company from facing any major consequences by monitoring patterns and sentiment in the news, customer feedback, and more.</p> <p>Overall, self-supervised learning can be implemented in any field to enhance the results of individuals and help companies reach their goals faster and as efficiently as possible.</p> <p><strong>Benefits</strong></p> <p>There are many great attributes self-supervised learning brings to the table that other machine learning processes are unable to do themselves. When training data, we have to choose which data is best fit for the model. Unfortunately, a vast majority of data is unlabeled, which is not ideal for training a model unless it can be cleaned easily. Self-supervised learning acts as the key to being able to “unlock” the use of the unlabeled data, and utilize its potential. It allows us to use the readily available unlabeled data instead of looking for scarce, hard to find labeled data, which even when it is found, is not found in amounts as large as unlabeled data is.</p> <p>Unlabeled data provides us with an abundance of information, but that bounty of data is proportional to its inefficiency. The cleanup and management of this amount of data is extremely lengthy and prone to human error. Self-supervised learning saves that time spent in cleaning the data for humans and allows us to focus on other important responsibilities, such as analysis. It also improves data accuracy by preventing human errors such as incorrect data entry, which is very difficult to find and correct if it happens with datasets so large.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HceW6I-RRoLoGFLb"/></figure> <p><strong><em>Figure 5: </em></strong><em>This is a representation of labeled (red dots) and unlabeled data (other dots). When a model is trained only on the labeled (red dots), the model will be limited to learning from a confined space. Instead, if the model is trained on unlabeled data (other dots) the model will have an enormous amount of knowledge and understanding about the data, then with that knowledge, it can guide it by using the label data toward a more specific area. The red dots serve as a guide for the other dots.</em></p> <p>While reducing time spent cleaning data and getting it ready for training, self-supervised learning also greatly reduces the money cost of the data cleaning. Manually cleaning data of this capacity requires a great amount of money and expertise to do so. Instead of paying humans to do this monotonous, yet very important, task for unprecedented amounts of time, the machine can do the cleaning itself while humans can dedicate their time towards tasks that cannot be accomplished by machines.</p> <p>Self-supervised learning is also known to be very scalable and adaptable across domains. It obtains this versatility because of its strong ability to find patterns in such severely unorganized data allows them to implement those patterns in any type of domain without too much customization. Its scalability ranges from any dataset size, and the same concepts can be applied to any dataset regardless of its magnitude.</p> <p><strong>How will it affect our future?</strong></p> <p>Even these days self-supervised learning has a great impact on artificial intelligence, like in the training of LLMs and computer vision models. As more development is toward improving these self-supervised learning frameworks the new model will achieve higher performance. As more people use technology more unlabeled data will be generated, and the improvement on these SSL frameworks will be able to use these data more efficiently and as a consequence will be a more useful model that will augment our capacities and improve human condition.</p> <p><strong>Conclusion</strong></p> <p>In conclusion, Self-Supervised learning is a framework designed to make a model or system learn from unlabeled. Self-supervised learning is a phase where the model learns general information about a topic and then is given to perform a general task by using Supervised learning where it uses label data.</p> <p>Work cited:</p> <ol><li><strong>Huyen, Chip.</strong> “RLHF: Reinforcement Learning from Human Feedback.” <em>huyenchip.com</em>, 2 May 2023. <a href="https://huyenchip.com/2023/05/02/rlhf.html">https://huyenchip.com/2023/05/02/rlhf.html</a></li><li><strong>Chen, Ting, et al.</strong> “A Simple Framework for Contrastive Learning of Visual Representations.” <em>arXiv</em>, 11 Feb. 2020. <a href="https://arxiv.org/pdf/2002.05709">https://arxiv.org/pdf/2002.05709</a></li><li><strong>Meta AI.</strong> “Self-Supervised Learning: The Dark Matter of Intelligence.” <em>ai.meta.com</em>, 4 Mar. 2021. <a href="https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/</a></li><li><strong>Boesch, Gaudenz.</strong> “Self-Supervised Learning: Everything You Need to Know (2024).” <em>viso.ai</em>, 24 Oct. 2023. <a href="https://viso.ai/deep-learning/self-supervised-learning-for-computer-vision/">https://viso.ai/deep-learning/self-supervised-learning-for-computer-vision/</a></li><li><strong>Bgn, Jonathan.</strong> “The Rise of Self-Supervised Learning.” <em>jonathanbgn.com</em>, 31 Dec. 2020. <a href="https://jonathanbgn.com/2020/12/31/self-supervised-learning.html">https://jonathanbgn.com/2020/12/31/self-supervised-learning.html</a></li><li><strong>Neptune.ai.</strong> “Self-Supervised Learning and Its Applications.” <em>neptune.ai</em>, <a href="https://neptune.ai/blog/self-supervised-learning">https://neptune.ai/blog/self-supervised-learning</a></li><li><strong>Tajia, Pedro.</strong> “Bootstrap Your Own Latent: Self-Supervised Learning Without Contrastive Learning.” <em>pedrotajia.com</em>, 20 Oct. 2024. <a href="https://pedrotajia.com/2024/10/20/bootstrap-your-own-latent.html">https://pedrotajia.com/2024/10/20/bootstrap-your-own-latent.html</a></li></ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5d00fa1c8b8e" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>