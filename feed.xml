<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://pedrotajia.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://pedrotajia.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-28T04:13:13+00:00</updated><id>https://pedrotajia.com/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Contrastive Deep Explanations</title><link href="https://pedrotajia.com/blog/2024/Contrastive-Deep-Explanations/" rel="alternate" type="text/html" title="Contrastive Deep Explanations"/><published>2024-11-24T00:00:00+00:00</published><updated>2024-11-24T00:00:00+00:00</updated><id>https://pedrotajia.com/blog/2024/Contrastive-Deep-Explanations</id><content type="html" xml:base="https://pedrotajia.com/blog/2024/Contrastive-Deep-Explanations/"><![CDATA[<script type="text/javascript" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <h2 id="1-introduction">1. Introduction</h2> <p>As the creation of large deep learning models has advanced, researchers have become curious about what happens inside these models. Even though many people use these models in their daily basis like grammar checking, self-driving cars, weather prediction, or more specialized areas such as cancer detection or predicting a protein’s 3D structure from its amino acid sequence, etc., nobody really knows how these models work internally. In cases where deep learning is used on medicine or self-driving cars, it is particularly important to know the reasoning behind the model’s decisions—for example, understanding why did not the model chose prediction <strong>B</strong> over the prediction <strong>A</strong>.</p> <p>Note: I will interchangeably use model or a neural network.</p> <p>In this article, I will explain the paper <a href="https://rlair.cs.ucr.edu/papers/docs/cdeepex.pdf">CDeepEx: Contrastive Deep Explanations</a>, which introduces a method capable of answer the question of <em>Why did you not choose answer B over A</em> and provided an overview of the concepts that a network learn.</p> <p><img src="/assets/contrastive-deep-explanations/Contrastive_example.svg" alt="Contrastive Example"/></p> <p><strong>Figure 1</strong>: This is an example of a classifier trained on the MNIST dataset. Its input is an image of a handwritten digit, for example the number <strong>3</strong> and the model predicts this number with a confidence of <em>72%</em>. The paper address the question of *Why the classifier predicts the number <strong>3</strong> instead of the number <strong>9*</strong> by showing how the image 3 can be transformed to make the classifier predict this new image as the number <strong>9</strong>.</p> <h2 id="2-cdeepex-contrastive-deep-explanations">2. CDeepEx: Contrastive Deep Explanations</h2> <p>To archive contrastive explanation i.e., to answer <em>Why did you not choose answer B over A</em>, the proposed method uses a generative latent-space model. This involves using a Wasserstein Generative Adversarial Networks (WGAN) or Variational AutoEncoder (VAE). The models learn a latent-space of the data, where is captures the fundamental information that compose the data. The latent-space can be viewed as a bridge between the network and human understanding of the data. The idea is to use a WGAN or VAE to learn this latent-space for later than be used to generate images that can explain a model’s reasoning process.</p> <p><img src="/assets/contrastive-deep-explanations/GAN-VAE.svg" alt="GAN-VAE"/></p> <p><strong>Figure 2</strong>: These are two generative models. (a) is a variational Autoencoder (VAE), where an image is inputted on the encoder, and using a <strong>code</strong>, it generates a latent representation where the input (image) is transformed into a lower- dimension that preserves the essential information of the input. The decoder uses this latent representation to reconstruct an image that is similar to the input. (b) Is a Wasserstein GAN (WGAN) where <strong>random noise</strong> is used to generate images that look similar from the data, and a discrimination is used to predict how real an image it is.</p> <p>Note: The latent representation is simply a point in the latent space. Remember that the goal of the WGAN or VAE is to create this latent space which contains the information the model learn from the data.</p> <p>The <strong>code</strong> (for VAE) and the <strong>random noise</strong> (for WGAN) can be viewed as the latent space. Since both the code and the random noise comes from a normal distribution is possible to sample a point from normal distribution and inputted in the decoder (for the VAE) and the generator (for the WGAN) to generate an image. The only component used to generate the explications is the <strong>decoder</strong> in the VAE and the <strong>generator</strong> in the WGAN.</p> <p>To generate explication is use a generator (a network that generate natural images) and the discriminator (the classifier of interest). The image $\mathcal{I}$ is inputted into the discriminator network $D$ to produce $y_{true}$. The class label of interest will be denoted as $y_{probe}$. Thus, we can formulate the question of <em>Why did $D$ produce label $y</em>{true}$ and not label $y_{probe}$ for the input $\mathcal{I}$?_.</p> <p>To generate explanation: <img src="/assets/contrastive-deep-explanations/Generate_explanation.png" alt="Algorithm_1-Algorithm_2"/></p> <p>In these algorithms, the first step is to train a generator $G$ that, given a latent representation of real values of size $k$, outputs an image of size $n$. After training the generator, we need to find a representation $z_0$ that, when inputted to $G$ generates an image similar to $\mathcal{I}$. Initially, $z_0$ is sampled from a normal distribution $\mathcal{N}(0,1)$ with mean 0 with variance of 1. We then iterate until the generated image $G(z)$ is close to $\mathcal{I}$. Inside the loop, $z_0$ is updated by gradient decent.</p> <p>After finding the correct latent representation $G(z_0)$ such that generates an image similar to $\mathcal{I}$, we get $\Delta_{z0}$ as the difference between $G$ and $\mathcal{I}$.</p> <p>To find $z_e$: <img src="/assets/contrastive-deep-explanations/Getting_Ze.png" alt="Getting_Ze"/></p> <p>After finding $z_e$ that minimizes L2 distance between $z$ and $z_0$, and that is in between the constraints, we compute the difference between $G(z_0) - G(z_e)$. This is done because we want to find a latent vector $z_e$ such that the resultant image has a similar style to the generated image from $z_0$, but is classified as our label of interest. By taking the difference between $G(z_0) - G(z_e)$, the overlapping parts are unchanged and parts that are different stands out.</p> <p><img src="/assets/contrastive-deep-explanations/Proposed_approach.svg" alt="proposed_method"/> <strong>Figure 3</strong>: An alternative way to the working the algorithm 1 and 2.</p> <h3 id="21-another-way-to-see-it">2.1. Another way to see it</h3> <p>The suggested methods work well on the MNIST dataset, showing the transformation needed for <em>Image A</em> classified as the <em>Number 8</em> to be classified as the <em>Number 3</em>. In the experiment is show different pair of number and the transformation need to be classified into different class. <img src="/assets/contrastive-deep-explanations/Figure2_mnist_experiment.png" alt="Figure2_mnist_experiment"/></p> <p>Instead of representing the transformation as red or blue for regions that should be added or removed, the transformations are represented as a timeline that shows the sequence of transformation needed to covert <em>Image 9</em> to converted into <em>Image 3</em>.</p> <p><img src="/assets/contrastive-deep-explanations/New_Approach.svg" alt="New proposed_method"/> <strong>Figure 4</strong>: The framework to view the problem differently. (a) Use a VAE (Decoder) or WGAN (Generator) to generate images. Start with an <em>image 9</em> from the MNIST dataset and update the latent vector $z$ to be close to this image, obtaining $z_0$. (b) The updated latent vector $z_0$ generates an image classified as <em>class 9</em>. We update the latent vector $z_0$ to get $z_e$ which, when generated, is predicted by the classifier as the <em>class 3</em>. During the process to update $z_0$ from <em>Image 9</em> to <em>Image 3</em>, we got these sequence of transformations.</p> <p><strong>Figure 4</strong>: Part (a)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">learn_z0</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">z0</span><span class="p">):</span>
    <span class="c1"># G is the generator
</span>    <span class="c1"># I is the image from a dataset
</span>    <span class="c1"># lr: Learning rate for the optimizer (default: 0.0005)
</span>    <span class="c1"># z0 the random initialized latent vector
</span>    <span class="c1"># set the optimizer "Adam" to optimize the loss with respect to the latent variable z0
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span><span class="n">z0</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

    <span class="c1"># Iterate over epochs
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

        <span class="c1"># set the calculation of the gradients for z0
</span>        <span class="n">z0</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>

        <span class="c1"># generate the image from z0
</span>        <span class="n">G_z0</span> <span class="o">=</span> <span class="nc">G</span><span class="p">(</span><span class="n">z0</span><span class="p">)</span>

        <span class="c1"># Calculate the loss (the loss is the norm l2 squared)
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">G_z0</span><span class="p">,</span> <span class="n">I</span><span class="p">)</span>

        <span class="c1"># backpropagate the error and get the gradients
</span>        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

        <span class="c1"># update z0
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">z0</span><span class="p">,</span>
</code></pre></div></div> <p><strong>Figure 4</strong>: Part (b)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">learn_ze</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span> <span class="n">some_pixel_threshold</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="c1"># G: Generator model
</span>    <span class="c1"># D: Discriminator model
</span>    <span class="c1"># epochs: Number of training iterations
</span>    <span class="c1"># z: Latent vector (input noise for the generator)
</span>    <span class="c1"># y: Target labels for the discriminator's output
</span>    <span class="c1"># lr: Learning rate for the optimizer (default: 0.0005)
</span>
    <span class="c1"># some_pixel_threshold: Threshold for pixel difference to store generated images (default: 5)
</span>
    <span class="c1"># Ensure the latent vector requires gradient computation
</span>    <span class="n">z</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="c1"># Initialize the Adam optimizer to update the latent vector z
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span><span class="n">z</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

    <span class="c1"># List to store generated images that meet the pixel difference criterion
</span>    <span class="n">grid_images</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Variable to store the previously generated image for comparison
</span>    <span class="n">prev_stored_image</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="c1"># Training loop over the specified number of epochs
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># Generate an image from the latent vector z and pass it through the discriminator
</span>        <span class="n">D_z</span><span class="p">,</span> <span class="n">G_z_resize</span> <span class="o">=</span> <span class="nf">discriminator_gen</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>

        <span class="c1"># Compute the cross-entropy loss between the discriminator's output and the target labels
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">D_z</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># Backpropagate the loss to compute gradients
</span>        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

        <span class="c1"># Update the latent vector z using the optimizer
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># Check if there's a previously stored image to compare with
</span>        <span class="k">if</span> <span class="n">prev_stored_image</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># Calculate the pixel-wise difference between the current and previous images
</span>            <span class="n">pixel_diff</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">G_z_resize</span> <span class="o">-</span> <span class="n">prev_stored_image</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span>
            <span class="c1"># If the difference exceeds the threshold, store the current image
</span>            <span class="k">if</span> <span class="n">pixel_diff</span> <span class="o">&gt;</span> <span class="n">some_pixel_threshold</span><span class="p">:</span>
                <span class="n">grid_images</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">G_z_resize</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
                <span class="c1"># Update the previous stored image to the current one
</span>                <span class="n">prev_stored_image</span> <span class="o">=</span> <span class="n">G_z_resize</span><span class="p">.</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If no previous image exists, store the current image
</span>            <span class="n">grid_images</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">G_z_resize</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
            <span class="c1"># Set the previous stored image to the current one
</span>            <span class="n">prev_stored_image</span> <span class="o">=</span> <span class="n">G_z_resize</span><span class="p">.</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">()</span>

    <span class="c1"># Return the optimized latent vector and the list of stored images
</span>    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">grid_images</span>
</code></pre></div></div> <p>I believe this view of the problem will be useful to understand how the network classifier goes through the latent space to find the image the outputs the correct label. Instead of depending on two variables to change an <em>image A</em> to an <em>image B</em>, is shown a sequence of transformation applied to <em>image A</em> to become <em>image B</em>.</p> <h2 id="3-related-work">3. Related Work</h2> <p>Several methods are made to interpret deep learning models. These methods can be segmented based on their approach to understanding model decisions:</p> <h3 id="31-network-visualizers">3.1. Network Visualizers</h3> <p>This method has the goal to understand the knowledge of a network by looking at individual neuron or group of neurons. By analyzing each neuron in a network we want to find features, like edges or textures that influence the predictions of a network. However, is uncommon to find features that have a dedicated neuron to it, instead we analyze a group of neurons, which give a more understandable insight of a network.</p> <h3 id="32-input-space-visualizers">3.2. Input Space Visualizers</h3> <p>Input space visualizers focus on explaining which parts of an image have the largest impact in a network decision. These methods are archived by modifying the input and observing how the output changes. Methods like xGEMs uses a GAN to find a contrastive example where tries to find a <em>why not</em> explication, however, in this method there is no formulation of a constrained optimization that give a more coherent explanation between <em>Image A</em> (original image) and <em>Image B</em> (Desired image).</p> <h3 id="33-justification-based-methods">3.3. Justification-Based Methods</h3> <p>These methods generates human-like like textual or visual to justify a network classification. While this methods give an easy way to understand a network decision, they do not always reflect the classification made by a network. Instead, it gives what humans expect to hear.</p> <p>The main advantage of <strong>CDeepEx</strong> is that do not rely on of modifying the network or using heuristics. Instead, we use generative latent-space models, were by using the latent-space as a bridge between network understanding and human understanding we produce explanation in the form of natural-looking images.</p> <h2 id="4-remarks">4. Remarks</h2> <p>In the paper CDeepEx provides a method for generating contrastive explanation, which can be used to understand why a model predicts the <em>class A</em> over <em>class B</em> for the <em>image A</em>. Also in the paper is shown interesting results. This includes:</p> <ul> <li>A detail analysis on the MNIST dataset.</li> <li>The <strong>selection of the generator model</strong>, where is shown the impact in the performance with different datasets with respect of using a VAE or WGAN.</li> <li>An analysis of biased MNIST, were is tested if the method can provide clear explanations for a bias classifier.</li> <li>Also, this method is tested on the CelebA dataset (a dataset with celebrities faces) and Fashion MNIST, which shows how robust the method is for more complex datasets.</li> </ul> <p>I encourage you to read the paper for more details.</p> <p>Thanks for reading!</p> <p>Cite as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{
  tajia2024contrastive,
  title={Contrastive Deep Explanations},
  author={Tajia, Pedro},
  year={2024},
  howpublished={\url{https://pedrotajia.com/2024/11/24/Contrastive-Deep-Explanations.html}}
}
</code></pre></div></div> <h3 id="reference">Reference</h3> <ol> <li> <p>Feghahati, A., Shelton, C. R., Pazzani, M. J., &amp; Tang, K. (2021). CDeepEx: Contrastive Deep Explanations. ECAI 2020. <a href="https://rlair.cs.ucr.edu/papers/docs/cdeepex.pdf">PDF</a></p> </li> <li> <p>Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., &amp; Courville, A. (2017). Improved training of wasserstein GANs. ICML, 30, 5769–5779. <a href="https://arxiv.org/pdf/1701.07875">PDF</a></p> </li> <li> <p>Kingma, D. P., &amp; Welling, M. (2014). Auto-Encoding Variational Bayes. ICLR. <a href="https://arxiv.org/pdf/1312.6114">PDF</a></p> </li> </ol>]]></content><author><name>Pedro Tajia</name></author><category term="Explainable AI"/><category term="Deep Learning"/><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://pedrotajia.com/assets/contrastive-deep-explanations/Preview.svg"/><media:content medium="image" url="https://pedrotajia.com/assets/contrastive-deep-explanations/Preview.svg" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">Bootstrap Your Own Latent: Self-Supervised Learning Without Contrastive Learning</title><link href="https://pedrotajia.com/blog/2024/bootstrap-your-own-latent/" rel="alternate" type="text/html" title="Bootstrap Your Own Latent: Self-Supervised Learning Without Contrastive Learning"/><published>2024-10-20T00:00:00+00:00</published><updated>2024-10-20T00:00:00+00:00</updated><id>https://pedrotajia.com/blog/2024/bootstrap-your-own-latent</id><content type="html" xml:base="https://pedrotajia.com/blog/2024/bootstrap-your-own-latent/"><![CDATA[<script type="text/javascript" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <h1 id="introduction">Introduction</h1> <p>Supervised Learning is the top solution to train Convolutional Neural Network (CNN), which is the model made to solve computer vision like self-driving car, security, automatization, etc. As these computer vision tasks increase, the need to improve these systems also emerges.</p> <p>One of the main ways to improve these systems is through having more data and bigger models, but unfortunately by using supervised learning these solutions need labeled data (information that has been classified by humans), <strong>which makes them expensive and difficult to get</strong>.</p> <p>The bottleneck that limits the process of training is the process itself. Since supervised learning is dependent on label data which limits the amount of information the model are trained on. This represents a big limitation on the training of these models and a waste of unlabeled data. Current research on Self-Supervised Learning is able to <strong>train model without label data and take out the need of using Contrastive Learning</strong> which is the most common way to train model with Self-Supervised Learning.</p> <h1 id="self-supervised-learning">Self-Supervised Learning</h1> <p>Self-Supervised learning gained popularity on the training of Large Language models, by having the model learn from many unlabeled data which make the model learn the general structure of the words and their meaning. With that general knowledge then these pre-trained models are use for transfer learning to solve more specific take.</p> <h3 id="transfer-learning">Transfer Learning</h3> <blockquote> <p>Transfer learning is the process of using a model like CNNs trained on a large corpus of labeled data to gain a general structure and meaning of images and then use it to solve a more specific tasks. The use of large label data is to ensure the model learns a broad variety of images. Normally transfer learning is used when there is a limited amount of data, limited computational power or the improvement of performance. Even do it seems that transfer learning can solve the problem of using datasets with small label data still it does not. Since a key component of using transfer learning is its implementation, the data needs to be similar or close to the large corpus of data that was used to train the model.</p> </blockquote> <p>Since more research has been done on training CNNs with self-supervised learning, there has been new approaches to make able CNN learn from unlabeled data. One of these approaches was introduced by the paper called <a href="https://arxiv.org/pdf/2006.07733">bootstrap your own latent</a> where demonstrates that is not necessary of use contrastive learning approach for a self-supervised setting.</p> <h3 id="contrastive-learning">Contrastive Learning</h3> <p>The idea of contrastive learning is to make a model learn an embedding space that captures the essential information about its inputs including their structure and semantics.</p> <p><strong>Example:</strong> The model $f_\theta$ w have an image input of 224 pixels by 224 pixels which have $[24*24] = 576$ dimensions. The model outputs a vector that represents the input of $16$ dimensions which occupies 36 times less space than the image with almost the same information.</p> <p>This is done by training the model to output vector representations that are close for similar examples and farther apart when there are different examples. To train the model three type exampled we use: the anchor example (image as a reference), a positive example (image closely related to the anchor example) and a negative example (an image that is not related to the anchor example).</p> <p><strong>Example:</strong> Imagine the task to create a model that discriminate between animals and non-animals. The inputs for the model will be an image of a dog, cat and a watermelon. The <strong>anchor example $x^a$</strong>(dog), <strong>positive example $x^+$</strong> (cat) and the <strong>negative example $x^-$</strong> (watermelon). The model which has a CNN denoted as $f_\theta$ (CNN is the one that gets the structure and meaning of the image) and a projection $g_\theta$ (a projection head is applied to map the representations of $f_\theta$ to its loss function). When the image of a dog and a cat is imputed to the model it should output similar vector representations <img src="/assets/bootstrap-your-own-latent/CL-Explication-positive.svg" alt="Example of similar example"/></p> <p>And vice-versa when the negative example is inputted to the model the vector representation is completely different and far from the representation of the anchor image. <img src="/assets/bootstrap-your-own-latent/CL-Explication-negative.svg" alt="Example of different example"/></p> <p>The paper <a href="https://arxiv.org/pdf/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</a>(SimCLR) is the foundation on implementing contrastive methods for self-supervised learning. In the paper SimCLR was introduced a loss called <strong>NT-Xent</strong> which was originally inspired on the <strong>InfoNCE</strong> just having $\tau$ temperature variable as a modification.</p> <p><strong>InfoNCE</strong> <span style="font-size: 1.5em;">$\ell_{i,j} = - \log \frac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{2N} \mathbf{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k)/\tau)}$</span></p> <p>The InfoNCE loss will enforce $x^a$ and $x^+$ to be similar pairs and also enforce pairs that are different. The sim(.) function is a similarity metric which measures how a vector is similar against others. This metric is used to minimize the difference between positive pairs $(x^a, x^x)$ and maximize the distance between negative pairs $(x^a, x^-)$</p> <p>In summary, we can think of contrastive tasks as trying to generate similar representation for positive examples and different for negative examples.</p> <h1 id="bootstrap-your-own-latent-byol">Bootstrap Your Own Latent: BYOL</h1> <p>In contrastive learning positive examples are easy to obtain, but negative examples are difficult to get. Positive examples can be just a modified version of the anchor image. Negative examples can be difficult to get because we to define what this is different to an anchor and have enough similarity to make a challenging to the model without human intervention.</p> <p>For this reason there is research of self-supervised learning without contrastive learning. This is difficult because there is a need for negative example, if not what can stop the model of generating the same vector representation in contrast to an anchor example and positive example which is called <em>collapse</em>. For negative examples the model is forced to learn meaning representations for its inputs.</p> <p>In order to understand how <strong>BYOL</strong> archive self-supervised learning without contrastive methods let’s explore the main components of this self-supervised learning framework.</p> <p>BYOL have two neural networks, named as <em>online</em> and <em>target</em> networks that are able to interact to each other. The model is trained by the online network to predict the target network representation with the same image using different augmented views.</p> <p><img src="/assets/bootstrap-your-own-latent/Augmentation_1.svg" alt="First augmentation Example"/></p> <p>To generate this augmented views, we create 2 distortionated copies form an input image, by applying two sets of data augmentation operations. The transformation includes</p> <blockquote> <ul> <li>random cropping: a random patch of the image is selected, with an area uniformly sampled between 8% and 100% of that of the original image, and an aspect ratio logarithmically sampled between 3/4 and 4/3. This patch is then resized to the target size of 224 × 224 using bicubic interpolation;</li> <li>Random horizontal flip: optional left-right flip;</li> <li>color jittering: the brightness, contrast, saturation and hue of the image are shifted by a uniformly random offset applied on all the pixels of the same image. The order in which these shifts are performed is randomly selected for each patch;</li> <li>color dropping: an optional conversion to grayscale. When applied, output intensity for a pixel (r,g,b) corresponds to its luma component, computed as $0.2989r+ 0.5870g+ 0.1140b$;</li> <li>Gaussian blurring: for a 224 × 224 image, a square Gaussian kernel of size 23 ×23 is used, with a standard deviation uniformly sampled over $[0.1,2.0]$;</li> <li>solarization: an optional color transformation $x→x·1{x&lt;0.5}+ (1−x)·1{x≥0.5}$ for pixels with values in $[0,1]$.</li> </ul> </blockquote> <p><em>Credits: <a href="https://arxiv.org/pdf/2006.07733">Bootstrap your own latent: A new approach to self-supervised Learning</a></em></p> <p>These augmentations double the examples, if we have a batch of 32 images, we end up with 64 images per batch.<br/> <img src="/assets/bootstrap-your-own-latent/Augmentation_conbination.jpg" alt="Second augmentation Example"/></p> <p>Data augmentation is used to force the model to learn invariant representations which means that independently of the transformation imposed to an input the model will generate the same representations.</p> <p>The online network have parameters $\theta$ updated by back propagation and is made from three components: an encoder $f_{\theta}$, projector $g_{\theta}$ and predictor $q_{\theta}$. The target network have an encoder $f_{\xi}$ and projector $g_{\xi}$. The parameters $\xi$ of the target network are not updated by back propagation, but instead the model is updated by <em>Exponential Moving Average</em> (EMA) of the online parameters $\theta$. The parameters of the target network can be seen a <strong>smoothed version</strong> of the online network.</p> <p><span style="font-size: 1.2em;">${\xi}\longleftarrow{\tau}{\xi}+(1-\tau){\theta}$</span></p> <blockquote> <p>$\tau$ is the decay rate $T\in[0, 1]$</p> </blockquote> <p>The representation head uses a ResNet-50 for $f_{\theta}$ and $f_{\xi}$. The ResNet-50 receives the augmented image of size (224, 224, 3) and output a vector representation or a vector embedding of 2048-dimensional for the online network $y_{\theta}$ and for the target network $y_{\xi}^{‘}$. Then a projection head $g$ receives the vector $y$ and produces the final output for the target network $sq(z_{\xi}^{‘})$. $sg$ means stop gradient, which the parameters $\xi$ for the target network will not be updated by back-propagation. The output $z_{\theta}$ of the projection head $g_{\theta}$ of the online network is inputted to the prediction head $q_{\theta}$ which produces the final output $q_{\theta}(z_{\theta})$ of the online network. The projection and prediction heads consist of a linear layer with an input shape of 2048-dimensions and output size of 4096 followed by <strong>batch normalization</strong>, a non-linear function (ReLU) and a final layer with output of dimension 256.</p> <blockquote> <p>The projection and predictions heads are <em>multi-layer perceptron</em> (MLP)</p> </blockquote> <p><img src="/assets/bootstrap-your-own-latent/BYOL-Architecture.png" alt="Image of the architecture of BYOL, image from the original paper"/> <em>Credits: <a href="https://arxiv.org/pdf/2006.07733">Bootstrap your own latent: A new approach to self-supervised Learning</a></em></p> <h3 id="training">Training</h3> <p>BYOL is train to minimizes the similarity loss between $q_{\theta}(z_{\theta})$ and $sq(z_{\xi}^{‘})$. The loss function is defined as: <span style="font-size: 1.2em;"></span></p> \[\mathcal{L}_{\theta, \xi} \triangleq \left\| \overline{q_{\theta}(z_0)} - \overline{z'_{\xi}} \right\|_2^2 = 2 - 2 \cdot \frac{\langle q_{\theta}(z_0), z'_{\xi} \rangle}{\| q_{\theta}(z_0) \|_2 \cdot \| z'_{\xi} \|_2}\] <p>&lt;/span&gt;</p> <p>$q_{\theta}(z_{\theta})$ and $z_{\xi}^{‘}$ are normalized to be unit vectors,</p> \[\overline{q}_{\theta}(z_{\theta}) = \frac{q_{\theta}(z_{\theta})}{\| q_{\theta}(z_0)\|_2}\] <p>and</p> \[\overline{z}_{\xi}^{'} = \frac{z^{'}_{\xi}}{\| z_{\xi}^{'} \|_2}\] <p>. Then is applied a mean squared error between the normalized outputs of the online and target networks.</p> <p>The loss</p> \[\mathcal{L}_{\theta,\xi}\] <p>is computed from feeding $v$ to the online network and $v’$ to the target network. The loss is symmetrized by calculating</p> \[\tilde{\mathcal{L}}_{\theta,\xi}\] <p>by feeding $v’$ to the online network and $v$ to the target network. <br/> <span style="font-size: 1.2em;"></span></p> \[\mathcal{L}^{BYOL}_{\theta, \xi} = \mathcal{L}_{\theta,\xi} + \tilde{\mathcal{L}}_{\theta,\xi}\] <p>&lt;/span&gt;</p> <p>The symmetrization of the loss makes each network, online and target have the same data to learn from. Since both networks share the same data it ensures that will have an equal contribution to the total loss. This promotes more robust and generalized features, since the model captures a wider range of data variations. <img src="/assets/bootstrap-your-own-latent/Symmetry_loss.svg" alt="An illustration about symmetrization of the loss"/></p> <p>For each training step is performed a $optimatizer$ algorithm to minimize $\mathcal{L}^{BYOL}_{\theta, \xi}$ with respect only to $\theta$. <span></span></p> \[{\theta}\longleftarrow\text{optimizer}(\theta, \nabla_{\theta}{\tilde{\mathcal{L}}_{\theta,\xi}}, {\eta})\] \[{\xi}\longleftarrow{\tau}{\xi}+(1-\tau){\theta}\] <p>&lt;/span&gt;</p> <blockquote> <p>$\eta$ is the learning rate</p> </blockquote> <p>In the framework BYOL the <a href="https://arxiv.org/pdf/1708.03888v3"><strong>LARS <em>optimizer</em></strong></a> is used update $\theta$, with a cosine decay learning rate schedule, more information on the <a href="https://arxiv.org/pdf/2006.07733">BYOL paper</a>. After the training, the encoder of the online network $f_{\theta}$ is used to produce representations.</p> <h2 id="why-byol-do-not-collapse">Why BYOL do not collapse</h2> <p>These are the two main reasons why BYOL do not collapse.</p> <p>In the paper <a href="https://arxiv.org/abs/2204.00613">On the Importance of Asymmetry for Siamese Representation Learning</a> explained the importance of the <strong>asymmetry designs</strong> (BYOL) in self-supervised frameworks. The representations outputted by the model improves when the <strong>source encoder</strong> in this case the online encoder it updated via gradient decent and the <strong>target encoder</strong> is updated by the source encoder weights. The outputs of target act as a judge of the quality of the output source. Also in the paper was proven in some level that <em>keeping a relatively lower variance in target encodings than source can help representation learning</em>. BYOL archive this low variance by updating the weight of the target network using EMA.</p> <p><img src="/assets/bootstrap-your-own-latent/Asymmetry_for_siamese.png" alt="Showing the importance of the variance between source and target encoders"/> <em>Credits: <a href="https://arxiv.org/abs/2204.00613">On the Importance of Asymmetry for Siamese Representation Learning</a></em></p> <p>In the post <a href="https://imbue.com/research/2020-08-24-understanding-self-supervised-contrastive-learning/">Understanding self-supervised and contrastive learning with “Bootstrap Your Own Latent” (BYOL)</a> is explained the importance of <strong>Batch Normalization</strong> in the prevention of <em>collapse</em>. They notice that if batch norm was not in the MLP the model will perform poorly. Batch norm standardize the activations in the network based on the batch’s mean and variance, which can vary between batches. Since the online and target network have different parameters in the batch norm layer, the output representation of the online and target network will also differ. These slightly differences in the outputs force the model to generate rich representations. However, is also highlighted that is worth avoiding batch normalization and use other alternatives like <strong>layer normalization</strong> or <strong>weight standardization with group normalization</strong>.</p> <h2 id="results">Results</h2> <p>The BYOL framework archive higher performance than the state-of-the-art contrastive methods in the ImageNet dataset. <img src="/assets/bootstrap-your-own-latent/Performance-of-BYOL-on-ImageNet.png" alt="Performance of BYOL on the ImageNet (linear evaluation)"/> <em>Credits: <a href="https://arxiv.org/pdf/2006.07733">Bootstrap your own latent: A new approach to self-supervised Learning</a></em></p> <p>BYOL is evaluated in both <strong>linear evaluation</strong> and <strong>fine-tuning evaluation</strong>. The linear evaluation consists on training a multinomial logistic regression on top of the frozen representations outputted by the encoder $f_\theta$ (The encoder weights are not trained in this evaluation.). <img src="/assets/bootstrap-your-own-latent/Linear_evaluation.svg" alt="An example of linear model"/></p> <p>To fine-tune evaluation consist on initialize $f_\theta$ parameters with the pre-trained representation, and retrain the encoder alongside a classifier on labeled dataset. <img src="/assets/bootstrap-your-own-latent/Fine-tuning.svg" alt="An example of fine-tune a model"/></p> <blockquote> <p>Note: For fine-tune there are many other types of architecture that can be used to fine tune this model.</p> </blockquote> <p>BYOL was pre-trained on ImageNet by 300 epochs. After pre-trained, the model is evaluated on many downstream tasks by using linear and fine-tune evaluations. <img src="/assets/bootstrap-your-own-latent/Table_3_result.png" alt="Table 3: Transfer learning results from ImageNet (IN) with the standard ResNet-50 architecture."/> <em>Credits: <a href="https://arxiv.org/pdf/2006.07733">Bootstrap your own latent: A new approach to self-supervised Learning</a></em> This result show competitive results to the Supervised training of RestNet-50 in ImageNet and surpass the performance of contrastive learning models.</p> <p>In these tables shows the robustness of BYOL against batch size compared to SimCLR. Also show the robustness for data augmentations showing that is not that sensitive to the choice of image augmentation like SimCLR. <img src="/assets/bootstrap-your-own-latent/Figure_2.png" alt="Figure 3: Decrease in top-1 accuracy (in % points) of BYOL and our own reproduction of SimCLR at 300 epochs, under linear evaluation on ImageNet."/> <em>Credits: <a href="https://arxiv.org/pdf/2006.07733">Bootstrap your own latent: A new approach to self-supervised Learning</a></em></p> <h2 id="remarks">Remarks</h2> <p>BYOL gives another solution to the traditional use of contrastive loss for self-supervised frameworks. Giving more research on this non-contrastive framework opens the door for more powerful models. Also taking out the need of negative examples gives more freedom to the model to understand the data and give richer representations.</p> <p>In the original BYOL paper have many other interesting topics:</p> <ul> <li>Result on linear and semi-supervised evaluation on ImageNet.</li> <li>A more detail information about the setup of BYOL.</li> <li>Details on the relation to contrastive methods.</li> <li>Pseudo-code in JAX to implement BYOL.</li> </ul> <p>I encourage you to look the papers from the reference section. This will give you a broader perspective on the <a href="https://arxiv.org/pdf/2006.07733">Bootstrap your own latent: A new approach to self-supervised Learning</a> paper.</p> <p>Thank you for reading!</p> <p>Cite as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{
  tajia2024bootstrap,
  title={Bootstrap Your Own Latent},
  author={Tajia, Pedro},
  year={2024},
  howpublished={\url{https://pedrotajia.com/2024/10/20/bootstrap-your-own-latent.html}}
}
</code></pre></div></div> <h2 id="references">References</h2> <ol> <li>Jean-Bastien Grill et al., <a href="https://arxiv.org/pdf/2006.07733">“Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning”</a>, arXiv, 2020.</li> <li>Ting Chen et al., <a href="https://arxiv.org/pdf/2002.05709">“A Simple Framework for Contrastive Learning of Visual Representations (SimCLR)”</a>, arXiv, 2020.</li> <li>Xinlei Chen and Kaiming He, <a href="https://arxiv.org/pdf/2204.00613">“On the Importance of Asymmetry for Siamese Representation Learning”</a>, arXiv, 2022.</li> <li>Imbue Research, <a href="https://imbue.com/research/2020-08-24-understanding-self-supervised-contrastive-learning/">“Understanding Self-Supervised and Contrastive Learning with ‘Bootstrap Your Own Latent’ (BYOL)”</a>, Blog post.</li> <li>Thalles Silva, <a href="https://sthalles.github.io/simple-self-supervised-learning/">“Exploring SimCLR: A Simple Framework for Contrastive Learning of Visual Representations”</a>, Blog post.</li> </ol>]]></content><author><name>Pedro Tajia</name></author><category term="Self Supervised"/><category term="Deep Learning"/><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://pedrotajia.com/assets/bootstrap-your-own-latent/BYOL-Architecture.png"/><media:content medium="image" url="https://pedrotajia.com/assets/bootstrap-your-own-latent/BYOL-Architecture.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://pedrotajia.com/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://pedrotajia.com/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://pedrotajia.com/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://pedrotajia.com/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://pedrotajia.com/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://pedrotajia.com/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website! 🎉🎉</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as sources.</p> <p>Any questions or suggestions? 👉 Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>