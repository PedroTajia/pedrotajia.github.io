<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Bootstrap Your Own Latent: Self-Supervised Learning Without Contrastive Learning | Pedro A. Tajia </title> <meta name="author" content="Pedro A. Tajia"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pedrotajia.com/blog/2024/bootstrap-your-own-latent/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Pedro</span> A. Tajia </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Bootstrap Your Own Latent: Self-Supervised Learning Without Contrastive Learning</h1> <p class="post-meta"> Created on October 20, 2024 by Pedro Tajia </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/self-supervised"> <i class="fa-solid fa-hashtag fa-sm"></i> Self Supervised</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <script type="text/javascript" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <h1 id="introduction">Introduction</h1> <p>Supervised Learning is the top solution to train Convolutional Neural Network (CNN), which is the model made to solve computer vision like self-driving car, security, automatization, etc. As these computer vision tasks increase, the need to improve these systems also emerges.</p> <p>One of the main ways to improve these systems is through having more data and bigger models, but unfortunately by using supervised learning these solutions need labeled data (information that has been classified by humans), <strong>which makes them expensive and difficult to get</strong>.</p> <p>The bottleneck that limits the process of training is the process itself. Since supervised learning is dependent on label data which limits the amount of information the model are trained on. This represents a big limitation on the training of these models and a waste of unlabeled data. Current research on Self-Supervised Learning is able to <strong>train model without label data and take out the need of using Contrastive Learning</strong> which is the most common way to train model with Self-Supervised Learning.</p> <h1 id="self-supervised-learning">Self-Supervised Learning</h1> <p>Self-Supervised learning gained popularity on the training of Large Language models, by having the model learn from many unlabeled data which make the model learn the general structure of the words and their meaning. With that general knowledge then these pre-trained models are use for transfer learning to solve more specific take.</p> <h3 id="transfer-learning">Transfer Learning</h3> <blockquote> <p>Transfer learning is the process of using a model like CNNs trained on a large corpus of labeled data to gain a general structure and meaning of images and then use it to solve a more specific tasks. The use of large label data is to ensure the model learns a broad variety of images. Normally transfer learning is used when there is a limited amount of data, limited computational power or the improvement of performance. Even do it seems that transfer learning can solve the problem of using datasets with small label data still it does not. Since a key component of using transfer learning is its implementation, the data needs to be similar or close to the large corpus of data that was used to train the model.</p> </blockquote> <p>Since more research has been done on training CNNs with self-supervised learning, there has been new approaches to make able CNN learn from unlabeled data. One of these approaches was introduced by the paper called <a href="https://arxiv.org/pdf/2006.07733" rel="external nofollow noopener" target="_blank">bootstrap your own latent</a> where demonstrates that is not necessary of use contrastive learning approach for a self-supervised setting.</p> <h3 id="contrastive-learning">Contrastive Learning</h3> <p>The idea of contrastive learning is to make a model learn an embedding space that captures the essential information about its inputs including their structure and semantics.</p> <p><strong>Example:</strong> The model $f_\theta$ w have an image input of 224 pixels by 224 pixels which have $[24*24] = 576$ dimensions. The model outputs a vector that represents the input of $16$ dimensions which occupies 36 times less space than the image with almost the same information.</p> <p>This is done by training the model to output vector representations that are close for similar examples and farther apart when there are different examples. To train the model three type exampled we use: the anchor example (image as a reference), a positive example (image closely related to the anchor example) and a negative example (an image that is not related to the anchor example).</p> <p><strong>Example:</strong> Imagine the task to create a model that discriminate between animals and non-animals. The inputs for the model will be an image of a dog, cat and a watermelon. The <strong>anchor example $x^a$</strong>(dog), <strong>positive example $x^+$</strong> (cat) and the <strong>negative example $x^-$</strong> (watermelon). The model which has a CNN denoted as $f_\theta$ (CNN is the one that gets the structure and meaning of the image) and a projection $g_\theta$ (a projection head is applied to map the representations of $f_\theta$ to its loss function). When the image of a dog and a cat is imputed to the model it should output similar vector representations <img src="/assets/bootstrap-your-own-latent/CL-Explication-positive.svg" alt="Example of similar example"></p> <p>And vice-versa when the negative example is inputted to the model the vector representation is completely different and far from the representation of the anchor image. <img src="/assets/bootstrap-your-own-latent/CL-Explication-negative.svg" alt="Example of different example"></p> <p>The paper <a href="https://arxiv.org/pdf/2002.05709" rel="external nofollow noopener" target="_blank">A Simple Framework for Contrastive Learning of Visual Representations</a>(SimCLR) is the foundation on implementing contrastive methods for self-supervised learning. In the paper SimCLR was introduced a loss called <strong>NT-Xent</strong> which was originally inspired on the <strong>InfoNCE</strong> just having $\tau$ temperature variable as a modification.</p> <p><strong>InfoNCE</strong> <span style="font-size: 1.5em;">$\ell_{i,j} = - \log \frac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{2N} \mathbf{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k)/\tau)}$</span></p> <p>The InfoNCE loss will enforce $x^a$ and $x^+$ to be similar pairs and also enforce pairs that are different. The sim(.) function is a similarity metric which measures how a vector is similar against others. This metric is used to minimize the difference between positive pairs $(x^a, x^x)$ and maximize the distance between negative pairs $(x^a, x^-)$</p> <p>In summary, we can think of contrastive tasks as trying to generate similar representation for positive examples and different for negative examples.</p> <h1 id="bootstrap-your-own-latent-byol">Bootstrap Your Own Latent: BYOL</h1> <p>In contrastive learning positive examples are easy to obtain, but negative examples are difficult to get. Positive examples can be just a modified version of the anchor image. Negative examples can be difficult to get because we to define what this is different to an anchor and have enough similarity to make a challenging to the model without human intervention.</p> <p>For this reason there is research of self-supervised learning without contrastive learning. This is difficult because there is a need for negative example, if not what can stop the model of generating the same vector representation in contrast to an anchor example and positive example which is called <em>collapse</em>. For negative examples the model is forced to learn meaning representations for its inputs.</p> <p>In order to understand how <strong>BYOL</strong> archive self-supervised learning without contrastive methods let’s explore the main components of this self-supervised learning framework.</p> <p>BYOL have two neural networks, named as <em>online</em> and <em>target</em> networks that are able to interact to each other. The model is trained by the online network to predict the target network representation with the same image using different augmented views.</p> <p><img src="/assets/bootstrap-your-own-latent/Augmentation_1.svg" alt="First augmentation Example"></p> <p>To generate this augmented views, we create 2 distortionated copies form an input image, by applying two sets of data augmentation operations. The transformation includes</p> <blockquote> <ul> <li>random cropping: a random patch of the image is selected, with an area uniformly sampled between 8% and 100% of that of the original image, and an aspect ratio logarithmically sampled between 3/4 and 4/3. This patch is then resized to the target size of 224 × 224 using bicubic interpolation;</li> <li>Random horizontal flip: optional left-right flip;</li> <li>color jittering: the brightness, contrast, saturation and hue of the image are shifted by a uniformly random offset applied on all the pixels of the same image. The order in which these shifts are performed is randomly selected for each patch;</li> <li>color dropping: an optional conversion to grayscale. When applied, output intensity for a pixel (r,g,b) corresponds to its luma component, computed as $0.2989r+ 0.5870g+ 0.1140b$;</li> <li>Gaussian blurring: for a 224 × 224 image, a square Gaussian kernel of size 23 ×23 is used, with a standard deviation uniformly sampled over $[0.1,2.0]$;</li> <li>solarization: an optional color transformation $x→x·1{x&lt;0.5}+ (1−x)·1{x≥0.5}$ for pixels with values in $[0,1]$.</li> </ul> </blockquote> <p><em>Credits: <a href="https://arxiv.org/pdf/2006.07733" rel="external nofollow noopener" target="_blank">Bootstrap your own latent: A new approach to self-supervised Learning</a></em></p> <p>These augmentations double the examples, if we have a batch of 32 images, we end up with 64 images per batch.<br> <img src="/assets/bootstrap-your-own-latent/Augmentation_conbination.jpg" alt="Second augmentation Example"></p> <p>Data augmentation is used to force the model to learn invariant representations which means that independently of the transformation imposed to an input the model will generate the same representations.</p> <p>The online network have parameters $\theta$ updated by back propagation and is made from three components: an encoder $f_{\theta}$, projector $g_{\theta}$ and predictor $q_{\theta}$. The target network have an encoder $f_{\xi}$ and projector $g_{\xi}$. The parameters $\xi$ of the target network are not updated by back propagation, but instead the model is updated by <em>Exponential Moving Average</em> (EMA) of the online parameters $\theta$. The parameters of the target network can be seen a <strong>smoothed version</strong> of the online network.</p> <p><span style="font-size: 1.2em;">${\xi}\longleftarrow{\tau}{\xi}+(1-\tau){\theta}$</span></p> <blockquote> <p>$\tau$ is the decay rate $T\in[0, 1]$</p> </blockquote> <p>The representation head uses a ResNet-50 for $f_{\theta}$ and $f_{\xi}$. The ResNet-50 receives the augmented image of size (224, 224, 3) and output a vector representation or a vector embedding of 2048-dimensional for the online network $y_{\theta}$ and for the target network $y_{\xi}^{‘}$. Then a projection head $g$ receives the vector $y$ and produces the final output for the target network $sq(z_{\xi}^{‘})$. $sg$ means stop gradient, which the parameters $\xi$ for the target network will not be updated by back-propagation. The output $z_{\theta}$ of the projection head $g_{\theta}$ of the online network is inputted to the prediction head $q_{\theta}$ which produces the final output $q_{\theta}(z_{\theta})$ of the online network. The projection and prediction heads consist of a linear layer with an input shape of 2048-dimensions and output size of 4096 followed by <strong>batch normalization</strong>, a non-linear function (ReLU) and a final layer with output of dimension 256.</p> <blockquote> <p>The projection and predictions heads are <em>multi-layer perceptron</em> (MLP)</p> </blockquote> <p><img src="/assets/bootstrap-your-own-latent/BYOL-Architecture.png" alt="Image of the architecture of BYOL, image from the original paper"> <em>Credits: <a href="https://arxiv.org/pdf/2006.07733" rel="external nofollow noopener" target="_blank">Bootstrap your own latent: A new approach to self-supervised Learning</a></em></p> <h3 id="training">Training</h3> <p>BYOL is train to minimizes the similarity loss between $q_{\theta}(z_{\theta})$ and $sq(z_{\xi}^{‘})$. The loss function is defined as: <span style="font-size: 1.2em;"> \(\mathcal{L}_{\theta, \xi} \triangleq \left\| \overline{q_{\theta}(z_0)} - \overline{z'_{\xi}} \right\|_2^2 = 2 - 2 \cdot \frac{\langle q_{\theta}(z_0), z'_{\xi} \rangle}{\| q_{\theta}(z_0) \|_2 \cdot \| z'_{\xi} \|_2}\) </span></p> <p>$q_{\theta}(z_{\theta})$ and $z_{\xi}^{‘}$ are normalized to be unit vectors, \(\overline{q}_{\theta}(z_{\theta}) = \frac{q_{\theta}(z_{\theta})}{\| q_{\theta}(z_0)\|_2}\) and \(\overline{z}_{\xi}^{'} = \frac{z^{'}_{\xi}}{\| z_{\xi}^{'} \|_2}\) . Then is applied a mean squared error between the normalized outputs of the online and target networks.</p> <p>The loss \(\mathcal{L}_{\theta,\xi}\) is computed from feeding $v$ to the online network and $v’$ to the target network. The loss is symmetrized by calculating \(\tilde{\mathcal{L}}_{\theta,\xi}\) by feeding $v’$ to the online network and $v$ to the target network. <br> <span style="font-size: 1.2em;"> \(\mathcal{L}^{BYOL}_{\theta, \xi} = \mathcal{L}_{\theta,\xi} + \tilde{\mathcal{L}}_{\theta,\xi}\) </span></p> <p>The symmetrization of the loss makes each network, online and target have the same data to learn from. Since both networks share the same data it ensures that will have an equal contribution to the total loss. This promotes more robust and generalized features, since the model captures a wider range of data variations. <img src="/assets/bootstrap-your-own-latent/Symmetry_loss.svg" alt="An illustration about symmetrization of the loss"></p> <p>For each training step is performed a $optimatizer$ algorithm to minimize $\mathcal{L}^{BYOL}_{\theta, \xi}$ with respect only to $\theta$. <span> \({\theta}\longleftarrow\text{optimizer}(\theta, \nabla_{\theta}{\tilde{\mathcal{L}}_{\theta,\xi}}, {\eta})\) \({\xi}\longleftarrow{\tau}{\xi}+(1-\tau){\theta}\) </span></p> <blockquote> <p>$\eta$ is the learning rate</p> </blockquote> <p>In the framework BYOL the <a href="https://arxiv.org/pdf/1708.03888v3" rel="external nofollow noopener" target="_blank"><strong>LARS <em>optimizer</em></strong></a> is used update $\theta$, with a cosine decay learning rate schedule, more information on the <a href="https://arxiv.org/pdf/2006.07733" rel="external nofollow noopener" target="_blank">BYOL paper</a>. After the training, the encoder of the online network $f_{\theta}$ is used to produce representations.</p> <h2 id="why-byol-do-not-collapse">Why BYOL do not collapse</h2> <p>These are the two main reasons why BYOL do not collapse.</p> <p>In the paper <a href="https://arxiv.org/abs/2204.00613" rel="external nofollow noopener" target="_blank">On the Importance of Asymmetry for Siamese Representation Learning</a> explained the importance of the <strong>asymmetry designs</strong> (BYOL) in self-supervised frameworks. The representations outputted by the model improves when the <strong>source encoder</strong> in this case the online encoder it updated via gradient decent and the <strong>target encoder</strong> is updated by the source encoder weights. The outputs of target act as a judge of the quality of the output source. Also in the paper was proven in some level that <em>keeping a relatively lower variance in target encodings than source can help representation learning</em>. BYOL archive this low variance by updating the weight of the target network using EMA.</p> <p><img src="/assets/bootstrap-your-own-latent/Asymmetry_for_siamese.png" alt="Showing the importance of the variance between source and target encoders"> <em>Credits: <a href="https://arxiv.org/abs/2204.00613" rel="external nofollow noopener" target="_blank">On the Importance of Asymmetry for Siamese Representation Learning</a></em></p> <p>In the post <a href="https://imbue.com/research/2020-08-24-understanding-self-supervised-contrastive-learning/" rel="external nofollow noopener" target="_blank">Understanding self-supervised and contrastive learning with “Bootstrap Your Own Latent” (BYOL)</a> is explained the importance of <strong>Batch Normalization</strong> in the prevention of <em>collapse</em>. They notice that if batch norm was not in the MLP the model will perform poorly. Batch norm standardize the activations in the network based on the batch’s mean and variance, which can vary between batches. Since the online and target network have different parameters in the batch norm layer, the output representation of the online and target network will also differ. These slightly differences in the outputs force the model to generate rich representations. However, is also highlighted that is worth avoiding batch normalization and use other alternatives like <strong>layer normalization</strong> or <strong>weight standardization with group normalization</strong>.</p> <h2 id="results">Results</h2> <p>The BYOL framework archive higher performance than the state-of-the-art contrastive methods in the ImageNet dataset. <img src="/assets/bootstrap-your-own-latent/Performance-of-BYOL-on-ImageNet.png" alt="Performance of BYOL on the ImageNet (linear evaluation)"> <em>Credits: <a href="https://arxiv.org/pdf/2006.07733" rel="external nofollow noopener" target="_blank">Bootstrap your own latent: A new approach to self-supervised Learning</a></em></p> <p>BYOL is evaluated in both <strong>linear evaluation</strong> and <strong>fine-tuning evaluation</strong>. The linear evaluation consists on training a multinomial logistic regression on top of the frozen representations outputted by the encoder $f_\theta$ (The encoder weights are not trained in this evaluation.). <img src="/assets/bootstrap-your-own-latent/Linear_evaluation.svg" alt="An example of linear model"></p> <p>To fine-tune evaluation consist on initialize $f_\theta$ parameters with the pre-trained representation, and retrain the encoder alongside a classifier on labeled dataset. <img src="/assets/bootstrap-your-own-latent/Fine-tuning.svg" alt="An example of fine-tune a model"></p> <blockquote> <p>Note: For fine-tune there are many other types of architecture that can be used to fine tune this model.</p> </blockquote> <p>BYOL was pre-trained on ImageNet by 300 epochs. After pre-trained, the model is evaluated on many downstream tasks by using linear and fine-tune evaluations. <img src="/assets/bootstrap-your-own-latent/Table_3_result.png" alt="Table 3: Transfer learning results from ImageNet (IN) with the standard ResNet-50 architecture."> <em>Credits: <a href="https://arxiv.org/pdf/2006.07733" rel="external nofollow noopener" target="_blank">Bootstrap your own latent: A new approach to self-supervised Learning</a></em> This result show competitive results to the Supervised training of RestNet-50 in ImageNet and surpass the performance of contrastive learning models.</p> <p>In these tables shows the robustness of BYOL against batch size compared to SimCLR. Also show the robustness for data augmentations showing that is not that sensitive to the choice of image augmentation like SimCLR. <img src="/assets/bootstrap-your-own-latent/Figure_2.png" alt="Figure 3: Decrease in top-1 accuracy (in % points) of BYOL and our own reproduction of SimCLR at 300 epochs, under linear evaluation on ImageNet."> <em>Credits: <a href="https://arxiv.org/pdf/2006.07733" rel="external nofollow noopener" target="_blank">Bootstrap your own latent: A new approach to self-supervised Learning</a></em></p> <h2 id="remarks">Remarks</h2> <p>BYOL gives another solution to the traditional use of contrastive loss for self-supervised frameworks. Giving more research on this non-contrastive framework opens the door for more powerful models. Also taking out the need of negative examples gives more freedom to the model to understand the data and give richer representations.</p> <p>In the original BYOL paper have many other interesting topics:</p> <ul> <li>Result on linear and semi-supervised evaluation on ImageNet.</li> <li>A more detail information about the setup of BYOL.</li> <li>Details on the relation to contrastive methods.</li> <li>Pseudo-code in JAX to implement BYOL.</li> </ul> <p>I encourage you to look the papers from the reference section. This will give you a broader perspective on the <a href="https://arxiv.org/pdf/2006.07733" rel="external nofollow noopener" target="_blank">Bootstrap your own latent: A new approach to self-supervised Learning</a> paper.</p> <p>Thank you for reading!</p> <p>Cite as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{
  tajia2024bootstrap,
  title={Bootstrap Your Own Latent},
  author={Tajia, Pedro},
  year={2024},
  howpublished={\url{https://pedrotajia.com/2024/10/20/bootstrap-your-own-latent.html}}
}
</code></pre></div></div> <h2 id="references">References</h2> <ol> <li>Jean-Bastien Grill et al., <a href="https://arxiv.org/pdf/2006.07733" rel="external nofollow noopener" target="_blank">“Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning”</a>, arXiv, 2020.</li> <li>Ting Chen et al., <a href="https://arxiv.org/pdf/2002.05709" rel="external nofollow noopener" target="_blank">“A Simple Framework for Contrastive Learning of Visual Representations (SimCLR)”</a>, arXiv, 2020.</li> <li>Xinlei Chen and Kaiming He, <a href="https://arxiv.org/pdf/2204.00613" rel="external nofollow noopener" target="_blank">“On the Importance of Asymmetry for Siamese Representation Learning”</a>, arXiv, 2022.</li> <li>Imbue Research, <a href="https://imbue.com/research/2020-08-24-understanding-self-supervised-contrastive-learning/" rel="external nofollow noopener" target="_blank">“Understanding Self-Supervised and Contrastive Learning with ‘Bootstrap Your Own Latent’ (BYOL)”</a>, Blog post.</li> <li>Thalles Silva, <a href="https://sthalles.github.io/simple-self-supervised-learning/" rel="external nofollow noopener" target="_blank">“Exploring SimCLR: A Simple Framework for Contrastive Learning of Visual Representations”</a>, Blog post.</li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Contrastive-Deep-Explanations/">Contrastive Deep Explanations</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Pedro A. Tajia. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>