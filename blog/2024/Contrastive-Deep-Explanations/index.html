<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Contrastive Deep Explanations | Pedro A. Tajia </title> <meta name="author" content="Pedro A. Tajia"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pedrotajia.com/blog/2024/Contrastive-Deep-Explanations/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Pedro</span> A. Tajia </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Contrastive Deep Explanations</h1> <p class="post-meta"> Created on November 24, 2024 by Pedro Tajia </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/explainable-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Explainable AI</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <script type="text/javascript" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <p align="center"> <img src="/assets/contrastive-deep-explanations/Preview.svg" style="width:80%;"> </p> <h2 id="1-introduction">1. Introduction</h2> <p>As the creation of large deep learning models has advanced, researchers have become curious about what happens inside these models. Even though many people use these models in their daily basis like grammar checking, self-driving cars, weather prediction, or more specialized areas such as cancer detection or predicting a protein’s 3D structure from its amino acid sequence, etc., nobody really knows how these models work internally. In cases where deep learning is used on medicine or self-driving cars, it is particularly important to know the reasoning behind the model’s decisions—for example, understanding why did not the model chose prediction <strong>B</strong> over the prediction <strong>A</strong>.</p> <p>Note: I will interchangeably use model or a neural network.</p> <p>In this article, I will explain the paper <a href="https://rlair.cs.ucr.edu/papers/docs/cdeepex.pdf" rel="external nofollow noopener" target="_blank">CDeepEx: Contrastive Deep Explanations</a>, which introduces a method capable of answer the question of <em>Why did you not choose answer B over A</em> and provided an overview of the concepts that a network learn.</p> <p align="center"> <img src="/assets/contrastive-deep-explanations/Contrastive_example.svg" style="width:80%;"> </p> <p> <strong>Figure 1</strong>: This is an example of a classifier trained on the MNIST dataset. Its input is an image of a handwritten digit, for example the number <strong>3</strong> and the model predicts this number with a confidence of <em>72%</em>. The paper address the question of *Why the classifier predicts the number <strong>3</strong> instead of the number <strong>9*</strong> by showing how the image 3 can be transformed to make the classifier predict this new image as the number <strong>9</strong>.</p> <h2 id="2-cdeepex-contrastive-deep-explanations">2. CDeepEx: Contrastive Deep Explanations</h2> <p>To archive contrastive explanation i.e., to answer <em>Why did you not choose answer B over A</em>, the proposed method uses a generative latent-space model. This involves using a Wasserstein Generative Adversarial Networks (WGAN) or Variational AutoEncoder (VAE). The models learn a latent-space of the data, where is captures the fundamental information that compose the data. The latent-space can be viewed as a bridge between the network and human understanding of the data. The idea is to use a WGAN or VAE to learn this latent-space for later than be used to generate images that can explain a model’s reasoning process.</p> <p align="center"> <img src="/assets/contrastive-deep-explanations/GAN-VAE.svg" style="width:80%;"> </p> <p><strong>Figure 2</strong>: These are two generative models. (a) is a variational Autoencoder (VAE), where an image is inputted on the encoder, and using a <strong>code</strong>, it generates a latent representation where the input (image) is transformed into a lower- dimension that preserves the essential information of the input. The decoder uses this latent representation to reconstruct an image that is similar to the input. (b) Is a Wasserstein GAN (WGAN) where <strong>random noise</strong> is used to generate images that look similar from the data, and a discrimination is used to predict how real an image it is.</p> <p>Note: The latent representation is simply a point in the latent space. Remember that the goal of the WGAN or VAE is to create this latent space which contains the information the model learn from the data.</p> <p>The <strong>code</strong> (for VAE) and the <strong>random noise</strong> (for WGAN) can be viewed as the latent space. Since both the code and the random noise comes from a normal distribution is possible to sample a point from normal distribution and inputted in the decoder (for the VAE) and the generator (for the WGAN) to generate an image. The only component used to generate the explications is the <strong>decoder</strong> in the VAE and the <strong>generator</strong> in the WGAN.</p> <p>To generate explication is use a generator (a network that generate natural images) and the discriminator (the classifier of interest). The image $\mathcal{I}$ is inputted into the discriminator network $D$ to produce $y_{true}$. The class label of interest will be denoted as $y_{probe}$. Thus, we can formulate the question of <em>Why did $D$ produce label $y</em>{true}$ and not label $y_{probe}$ for the input $\mathcal{I}$?_.</p> <p>To generate explanation:</p> <p align="center"> <img src="/assets/contrastive-deep-explanations/Generate_explanation.png" style="width:80%;"> </p> <p></p> <p>In these algorithms, the first step is to train a generator $G$ that, given a latent representation of real values of size $k$, outputs an image of size $n$. After training the generator, we need to find a representation $z_0$ that, when inputted to $G$ generates an image similar to $\mathcal{I}$. Initially, $z_0$ is sampled from a normal distribution $\mathcal{N}(0,1)$ with mean 0 with variance of 1. We then iterate until the generated image $G(z)$ is close to $\mathcal{I}$. Inside the loop, $z_0$ is updated by gradient decent.</p> <p>After finding the correct latent representation $G(z_0)$ such that generates an image similar to $\mathcal{I}$, we get $\Delta_{z0}$ as the difference between $G$ and $\mathcal{I}$.</p> <p>To find $z_e$:</p> <p align="center"> <img src="/assets/contrastive-deep-explanations/Getting_Ze.png" style="width:80%;"> </p> <p> After finding $z_e$ that minimizes L2 distance between $z$ and $z_0$, and that is in between the constraints, we compute the difference between $G(z_0) - G(z_e)$. This is done because we want to find a latent vector $z_e$ such that the resultant image has a similar style to the generated image from $z_0$, but is classified as our label of interest. By taking the difference between $G(z_0) - G(z_e)$, the overlapping parts are unchanged and parts that are different stands out.</p> <p align="center"> <img src="/assets/contrastive-deep-explanations/Proposed_approach.svg" style="width:80%;"> </p> <p> <strong>Figure 3</strong>: An alternative way to the working the algorithm 1 and 2.</p> <h3 id="21-another-way-to-see-it">2.1. Another way to see it</h3> <p>The suggested methods work well on the MNIST dataset, showing the transformation needed for <em>Image A</em> classified as the <em>Number 8</em> to be classified as the <em>Number 3</em>. In the experiment is show different pair of number and the transformation need to be classified into different class.</p> <p align="center"> <img src="/assets/contrastive-deep-explanations/Figure2_mnist_experiment.png" style="width:80%;"> </p> <p> Instead of representing the transformation as red or blue for regions that should be added or removed, the transformations are represented as a timeline that shows the sequence of transformation needed to covert <em>Image 9</em> to converted into <em>Image 3</em>.</p> <p align="center"> <img src="/assets/contrastive-deep-explanations/New_Approach.svg" style="width:80%;"> </p> <p> <strong>Figure 4</strong>: The framework to view the problem differently. (a) Use a VAE (Decoder) or WGAN (Generator) to generate images. Start with an <em>image 9</em> from the MNIST dataset and update the latent vector $z$ to be close to this image, obtaining $z_0$. (b) The updated latent vector $z_0$ generates an image classified as <em>class 9</em>. We update the latent vector $z_0$ to get $z_e$ which, when generated, is predicted by the classifier as the <em>class 3</em>. During the process to update $z_0$ from <em>Image 9</em> to <em>Image 3</em>, we got these sequence of transformations.</p> <p><strong>Figure 4</strong>: Part (a)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">learn_z0</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">z0</span><span class="p">):</span>
    <span class="c1"># G is the generator
</span>    <span class="c1"># I is the image from a dataset
</span>    <span class="c1"># lr: Learning rate for the optimizer (default: 0.0005)
</span>    <span class="c1"># z0 the random initialized latent vector
</span>    <span class="c1"># set the optimizer "Adam" to optimize the loss with respect to the latent variable z0
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span><span class="n">z0</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

    <span class="c1"># Iterate over epochs
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

        <span class="c1"># set the calculation of the gradients for z0
</span>        <span class="n">z0</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>

        <span class="c1"># generate the image from z0
</span>        <span class="n">G_z0</span> <span class="o">=</span> <span class="nc">G</span><span class="p">(</span><span class="n">z0</span><span class="p">)</span>

        <span class="c1"># Calculate the loss (the loss is the norm l2 squared)
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">G_z0</span><span class="p">,</span> <span class="n">I</span><span class="p">)</span>

        <span class="c1"># backpropagate the error and get the gradients
</span>        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

        <span class="c1"># update z0
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">z0</span><span class="p">,</span>
</code></pre></div></div> <p><strong>Figure 4</strong>: Part (b)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">learn_ze</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span> <span class="n">some_pixel_threshold</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="c1"># G: Generator model
</span>    <span class="c1"># D: Discriminator model
</span>    <span class="c1"># epochs: Number of training iterations
</span>    <span class="c1"># z: Latent vector (input noise for the generator)
</span>    <span class="c1"># y: Target labels for the discriminator's output
</span>    <span class="c1"># lr: Learning rate for the optimizer (default: 0.0005)
</span>
    <span class="c1"># some_pixel_threshold: Threshold for pixel difference to store generated images (default: 5)
</span>
    <span class="c1"># Ensure the latent vector requires gradient computation
</span>    <span class="n">z</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="c1"># Initialize the Adam optimizer to update the latent vector z
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span><span class="n">z</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

    <span class="c1"># List to store generated images that meet the pixel difference criterion
</span>    <span class="n">grid_images</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Variable to store the previously generated image for comparison
</span>    <span class="n">prev_stored_image</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="c1"># Training loop over the specified number of epochs
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># Generate an image from the latent vector z and pass it through the discriminator
</span>        <span class="n">D_z</span><span class="p">,</span> <span class="n">G_z_resize</span> <span class="o">=</span> <span class="nf">discriminator_gen</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>

        <span class="c1"># Compute the cross-entropy loss between the discriminator's output and the target labels
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">D_z</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># Backpropagate the loss to compute gradients
</span>        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

        <span class="c1"># Update the latent vector z using the optimizer
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># Check if there's a previously stored image to compare with
</span>        <span class="k">if</span> <span class="n">prev_stored_image</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># Calculate the pixel-wise difference between the current and previous images
</span>            <span class="n">pixel_diff</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">G_z_resize</span> <span class="o">-</span> <span class="n">prev_stored_image</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span>
            <span class="c1"># If the difference exceeds the threshold, store the current image
</span>            <span class="k">if</span> <span class="n">pixel_diff</span> <span class="o">&gt;</span> <span class="n">some_pixel_threshold</span><span class="p">:</span>
                <span class="n">grid_images</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">G_z_resize</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
                <span class="c1"># Update the previous stored image to the current one
</span>                <span class="n">prev_stored_image</span> <span class="o">=</span> <span class="n">G_z_resize</span><span class="p">.</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If no previous image exists, store the current image
</span>            <span class="n">grid_images</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">G_z_resize</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
            <span class="c1"># Set the previous stored image to the current one
</span>            <span class="n">prev_stored_image</span> <span class="o">=</span> <span class="n">G_z_resize</span><span class="p">.</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">()</span>

    <span class="c1"># Return the optimized latent vector and the list of stored images
</span>    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">grid_images</span>
</code></pre></div></div> <p>I believe this view of the problem will be useful to understand how the network classifier goes through the latent space to find the image the outputs the correct label. Instead of depending on two variables to change an <em>image A</em> to an <em>image B</em>, is shown a sequence of transformation applied to <em>image A</em> to become <em>image B</em>.</p> <h2 id="3-related-work">3. Related Work</h2> <p>Several methods are made to interpret deep learning models. These methods can be segmented based on their approach to understanding model decisions:</p> <h3 id="31-network-visualizers">3.1. Network Visualizers</h3> <p>This method has the goal to understand the knowledge of a network by looking at individual neuron or group of neurons. By analyzing each neuron in a network we want to find features, like edges or textures that influence the predictions of a network. However, is uncommon to find features that have a dedicated neuron to it, instead we analyze a group of neurons, which give a more understandable insight of a network.</p> <h3 id="32-input-space-visualizers">3.2. Input Space Visualizers</h3> <p>Input space visualizers focus on explaining which parts of an image have the largest impact in a network decision. These methods are archived by modifying the input and observing how the output changes. Methods like xGEMs uses a GAN to find a contrastive example where tries to find a <em>why not</em> explication, however, in this method there is no formulation of a constrained optimization that give a more coherent explanation between <em>Image A</em> (original image) and <em>Image B</em> (Desired image).</p> <h3 id="33-justification-based-methods">3.3. Justification-Based Methods</h3> <p>These methods generates human-like like textual or visual to justify a network classification. While this methods give an easy way to understand a network decision, they do not always reflect the classification made by a network. Instead, it gives what humans expect to hear.</p> <p>The main advantage of <strong>CDeepEx</strong> is that do not rely on of modifying the network or using heuristics. Instead, we use generative latent-space models, were by using the latent-space as a bridge between network understanding and human understanding we produce explanation in the form of natural-looking images.</p> <h2 id="4-remarks">4. Remarks</h2> <p>In the paper CDeepEx provides a method for generating contrastive explanation, which can be used to understand why a model predicts the <em>class A</em> over <em>class B</em> for the <em>image A</em>. Also in the paper is shown interesting results. This includes:</p> <ul> <li>A detail analysis on the MNIST dataset.</li> <li>The <strong>selection of the generator model</strong>, where is shown the impact in the performance with different datasets with respect of using a VAE or WGAN.</li> <li>An analysis of biased MNIST, were is tested if the method can provide clear explanations for a bias classifier.</li> <li>Also, this method is tested on the CelebA dataset (a dataset with celebrities faces) and Fashion MNIST, which shows how robust the method is for more complex datasets.</li> </ul> <p>I encourage you to read the paper for more details.</p> <p>Thanks for reading!</p> <p>Cite as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{
  tajia2024contrastive,
  title={Contrastive Deep Explanations},
  author={Tajia, Pedro},
  year={2024},
  howpublished={\url{https://pedrotajia.com/2024/11/24/Contrastive-Deep-Explanations.html}}
}
</code></pre></div></div> <h3 id="reference">Reference</h3> <ol> <li> <p>Feghahati, A., Shelton, C. R., Pazzani, M. J., &amp; Tang, K. (2021). CDeepEx: Contrastive Deep Explanations. ECAI 2020. <a href="https://rlair.cs.ucr.edu/papers/docs/cdeepex.pdf" rel="external nofollow noopener" target="_blank">PDF</a></p> </li> <li> <p>Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., &amp; Courville, A. (2017). Improved training of wasserstein GANs. ICML, 30, 5769–5779. <a href="https://arxiv.org/pdf/1701.07875" rel="external nofollow noopener" target="_blank">PDF</a></p> </li> <li> <p>Kingma, D. P., &amp; Welling, M. (2014). Auto-Encoding Variational Bayes. ICLR. <a href="https://arxiv.org/pdf/1312.6114" rel="external nofollow noopener" target="_blank">PDF</a></p> </li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/bootstrap-your-own-latent/">Bootstrap Your Own Latent: Self-Supervised Learning Without Contrastive Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Pedro A. Tajia. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>