<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>AI Can Learn From Their Dreams: World Models</p> <p>By: Pedro Tajia &amp; Syed Islam</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*X4f1INi-khcMteZ5"></figure> <h3>Introduction:</h3> <p>Since birth, humans have functioned as information processing machines through our senses with vision, audio, touch, etc. We learn to process this information naturally, analyzing and abstracting it to build what we call our “world model” or our understanding of “reality.” Our world model emerges from these limited senses and shapes how we perceive everything around us.</p> <p>Most people don’t directly process complex events like astronaut activities on the International Space Station. Instead, they build their reality through information from others who help make sense of these distant happenings. Think about reading this text right now — maybe you’re in your bedroom looking at your phone screen. This moment forms your current reality, and when you notice a car passing by your window, your mind naturally adds this new detail to your understanding. This mental picture of reality that grows in your mind becomes your personal “World Model.”</p> <p>Now imagine taking this idea of how we create and update our world model and applying it to AI, especially in Deep Learning systems. What possibilities might that open up? Well in this Article, we will take a deeper look into how we can apply the “World Model” idea into the world of AI.</p> <h3>World model in deep learning:</h3> <p>In order to apply the idea of the world model in deep learning (a subfield of AI that uses neural networks), the system has to learn about its environment.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MupvOfr1JE3HzanI"></figure> <p>Figure 1: A single frame from the car simulator (<a href="https://gymnasium.farama.org/environments/box2d/car_racing/" rel="external nofollow noopener" target="_blank">CarRacing-v3</a>).</p> <p>We want the model to understand the physics and behavior of this simulator. In the paper, “<a href="https://arxiv.org/abs/1803.10122" rel="external nofollow noopener" target="_blank">World Models</a>”, it shows that with a variational autoencoder (VAE) we can approximate this simulation.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Ih3dyECBxPhZBVkO"></figure> <p>Figure 2: This image shows VAE, an unsupervised deep learning algorithm used to learn representations from the data. Given an input, the encoder reduces the information into a vector representation and the decoder recovers the information from the vector into a reconstructed image.</p> <p>To train this VAE to gain an understanding of the environment the agent (the entity that interacts in the environment) a.k.a “the red car” collects the data. The agents act randomly to explore the environment for each action at and the resultant observation will be stored to train this VAE. 10,000 rollouts each rollout is the agent interacting in the environment where the agent is in the race for some time and then terminates.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qnd6GkLQUQhtYPCO"></figure> <p>Figure 3: A sequence of images from the car simulation with its <strong>action a </strong>at a <strong>time t </strong>or <strong>frame</strong>.</p> <p>This is the list of discrete actions available in the car simulation [ 0: do nothing, 1: steer left, 2: steer right, 3: gas, 4: brake]</p> <p>These frame observations are used to train the VAE to make the encoder represent each frame into a vector representation <em>z</em>, and then the decoder uses this vector <em>z</em> to reconstruct the original frame. The task of the VAE is to reduce the difference between the original and reconstructed frame, whereby doing this we can have correct vector representations of the observations/frames.</p> <p>When the VAE is already trained on the rollouts, we will use the <strong>encoder </strong>to produce vector representations z to train a Recurrent Neural Network(RNN), which serves as a predictive model for the future zt+1 vector which is put together with a Mixture Density Network to give this next z vector. With this, the RNN will generate the next vector zt+1 given the action at taken from the vector representation, and with the hidden state of the RNN ht more formally P(zt+1| at, zt, ht).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rrCzwiWAYbnEaJ6p"></figure> <p>Figure 4: A figure of the <a href="https://arxiv.org/pdf/1803.10122" rel="external nofollow noopener" target="_blank">original paper</a>. The RNN with the MDN (MDN-RNN) will generate a vector zt+1 from the past zt vector, action, at and hidden state ht.</p> <p>As the model learns to predict the next vector representation zt+1 given zt, at and ht the model will learn to analyze the past, understand the present, and be able to predict the future.</p> <p>The last part of the world model system is the controller or agent, which is the one that gives the hidden state at time t of an RNN ht and the vector z from the encoder given the observation the controller will perform some action at. The original paper explains that the controller is the smallest part of the world model, only having a small number of parameters as compared to the VAE and MDN-RNN, because the controller works in the vector space z and not on the observation, which makes it possible to have a small controller since it will not work in understanding the meaning of an image as the VAE does. The controller can be viewed as a linear function having at = Wc [zt , ht] + bc where Wc and bc are learnable parameters are learning by taking the action at given zt and ht that maximizes the expected cumulative reward.</p> <p>By combining the VAE, MDR-RNN, and the Controller, we will have the <strong>World Model</strong>. When all the parts of the world model are trained to give a single frame, we can generate a sequence of observations that the Controller can interact with, which means that the controller can learn from the generated vector representations from the MDR-RNN and can be seen from the decoder of the VAE.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*O7CELSjNzwID_2bR"></figure> <p>Figure 5: This figure shows a small sequence of dreamed examples. The first image is gathered from the original simulation and passed to an encoder, which is transferred into a latent vector. This vector is inputted to the <strong>controller C</strong>, taking action in this deemed simulation. The action is passed to the <strong>MDN-RNN</strong> where it returns a predicted reward and latent vector which the controller can use to take an action and improve itself by the given reward. This sequence of actions is finished when the controller decides to finish the simulation.</p> <h3>Benefits of the Word Model</h3> <p>Now that we have a good understanding on how World Models function in AI, let’s take a look at how they benefit us. The core advantage is pretty straightforward — World Models create simplified training environments that dramatically reduce computational requirements.</p> <p>For example, instead of training our AI agents directly with high-resolution images, World Models convert these images into what we call latent vectors, which are compressed versions of what the AI sees. Think of it like this: rather than dealing with every single pixel of a 4K image (which would be very computationally expensive), the World Model boils it down to just the essential information the AI needs.</p> <p>But here’s where it gets more beneficial — World Models can handle all sorts of different environments, not just visual ones. Want to simulate how a robot should move? World Models can help with that. Need to model complex business decisions? They’ve got you covered there. Even abstract problem-solving scenarios can be modeled this way. In each case, the World Model takes complicated information and turns it into a simplified version that’s much easier to work with while keeping all the important stuff needed for training intact.</p> <p>This versatility is what makes World Models so advantageous. And to see just how effective they can be, let’s take a look at the VizDoom experiment.</p> <h3>Application of a World Model (VizDoom Experiment)</h3> <p>The VizDoom experiment offers compelling evidence of World Models’ effectiveness. In this experiment, researchers trained an AI agent in a dream-like environment that simulated the VizDoom game, with the primary goal of determining whether skills learned in a simulated environment could transfer successfully to the actual game.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*0vzo6GF1b5uNiaT8"></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9uu6VOQ2GGg-3uJq"></figure> <p>Figure 6: Actual Game Environment Figure 7: Dream-like Environment</p> <p>The experiment’s setup was straightforward: the AI agent was placed in a closed room where it needed to dodge fireballs shot by monsters, with the objective of surviving as long as possible. What made the results particularly interesting was how well the agent adapted to its environment. Through training in the dream world, the agent developed realistic behaviors, learning to navigate without hitting the walls and avoid incoming fireballs effectively.</p> <p>Perhaps most remarkably, the researchers discovered that by increasing certain parameters to make the dream environment more challenging than the real game — thus making it harder for the agent to dodge fireballs — the agent’s performance in the actual game environment improved significantly. This finding demonstrates a key advantage of World Models: not only can they train agents with fewer computational resources, but they can potentially achieve better results than training in the real environment itself.</p> <p>This experiment serves as a powerful demonstration of World Models’ potential, showing that effective training can occur in simplified, dream-like environments while still producing superior real-world results.</p> <h3>Conclusion</h3> <p>Throughout this article, we explored the concept of World Models applied to AI. This exploration reveals a fundamental shift in how AI might learn and understand its environment. Just as humans build mental models of reality from their limited sensory inputs, these AI systems can now construct their own representations of the world around them. Through the combination of VAEs, MDN-RNNS, and simple controllers, we’ve shown that AI can not only perceive its environment but create internal simulations to learn from.</p> <p>However, there are still significant challenges that remain. Our current AI systems, despite their amazing capabilities, are constrained by memory limitations. Also, agents might exploit imperfections in their world model, achieving high scores in simulated environments that don’t translate to real-world success.</p> <p>The implications of the world model stretch beyond just technical achievements. World models could change AI development by lowering computational requirements and making advanced training methods accessible to smaller organizations. This shift could spark innovation across industries, create new opportunities in simulation-based training systems and complex decision-making applications.</p> <p>The idea of the world model prompts a deeper philosophical question about the nature of intelligence and consciousness. When an AI system can “dream” and learn from imagined scenarios, what does that tell us about the nature of understanding and learning? How do we ensure that behaviors learned in simulated environments align with real-world requirements?</p> <p>As we move forward, the key lies in balancing rapid advancement with responsible developments. World models represent more than just a technical achievement — they signal a fundamental shift in how AI might learn and understand its environment. The challenge ahead lies in nurturing growth while ensuring it remains aligned with human values and societal benefit.</p> <p>Source Links:</p> <ul> <li><a href="https://medium.com/@barrettnash/deepseeks-disruptive-debut-true-capitalism-in-action-sorry-trillion-dollar-oligarchs-cc82f536d544" rel="external nofollow noopener" target="_blank">https://medium.com/@barrettnash/deepseeks-disruptive-debut-true-capitalism-in-action-sorry-trillion-dollar-oligarchs-cc82f536d544</a></li> <li><a href="https://www.bbc.com/news/articles/c5yv5976z9po" rel="external nofollow noopener" target="_blank">https://www.bbc.com/news/articles/c5yv5976z9po</a></li> <li><a href="https://www.politico.com/newsletters/digital-future-daily/2025/01/27/whats-behind-the-deepseek-freakout-00200813" rel="external nofollow noopener" target="_blank">https://www.politico.com/newsletters/digital-future-daily/2025/01/27/whats-behind-the-deepseek-freakout-00200813</a></li> </ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3018fb21602b" width="1" height="1" alt=""></p> </body></html>