<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fw3MDu43-Iwgnm5NFmWHyQ.jpeg"></figure> <p>By: Rakshita Narayanaswamy</p> <p>Additional Contributions: Sahana Narayan, Pearl Vishen</p> <p>Word embeddings are a key concept in natural language processing (NLP), a field within machine learning that deals with understanding and generating human language. The communication tool Google Translate is a widely recognized example, leveraging these techniques to analyze the structure and meaning of sentences to provide translations. Let’s run an example of this:</p> <p>Take the sentence: “She is an architect. He is an interior decorator.”</p> <p>Let’s first translate it into Armenian.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*zqxn7ipXhmp1abnW"></figure> <p>Now, let’s translate this back to English:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*63prcOwWgSRQi7TP"></figure> <p>If we translate this sentence into a non-gendered language (Armenian) and then back to a gendered one (English), we might see the professions switch, with “He” becoming the architect and “She” the interior decorator. This demonstrates how word embeddings are utilized in a tool like Google Translate to conceptualize language and, unintentionally, propagate societal biases.</p> <h3><strong>What are Word Embeddings?</strong></h3> <p>Most machine-learning models are incapable of interpreting plain text in its raw form. Thus, word embeddings are treated like dictionaries for machines, having an algorithm translate a word into its numerical representation. In Natural Language Processing, words are represented as vectors in a high-dimensional space, forming a complex network of associations. Imagine this space as a maze of connections, where each path between words represents a semantic relationship. Words with similar meanings or contextual usage–like child and infant–are positioned closer together, while unrelated words are further apart.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*vG0cRJ6qW_7CIIh4"><figcaption>Figure 1: Semantic Feature subspace comparing age to gender [3]</figcaption></figure> <p>Through capturing the hidden landscapes within language, certain paths (like the professions “architect” and “interior decorator”) become linked with gendered ‘directions’, demonstrating the inherent biases within the data it’s trained on.</p> <p>Modern models are trained on a copious amount of text data — typically made up of journalistic, social media, or other culturally sourced texts. As a result, these language models operate similarly to a ‘mirror’, capturing and amplifying the cultural attitudes and stereotypes embedded within society. When NLP models use these embeddings for tasks like translation, these biases can surface, as we saw in the example above. Google Translate relies on contextual word embeddings that analyze entire sentences to capture word relationships within the text to generate the embedding. As such, when asked to re-assign pronouns and fill contextual gaps, the model relied on the associations it learned to predict the most statistically probable option-making gender-based choices rather than neutral ones.</p> <h3><strong>Data: Closer Inspection of Professional Bias</strong></h3> <p>Profession bias is one of the most evident forms of gender disparities. In a 2016 study at Cornell University, Bolukbasi and his team were one of the first to identify this problematic relationship. The researchers explored simple analogies and comparisons regarding occupations or gender roles, and heavy stereotypes surfaced: for instance, “man” is closer to “doctor,” and “woman” is closer to “nurse” — or “computer programmer” is to “man” as “homemaker” is to “woman” [1].</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/828/0*-QtMk4Jzer0l4RwY"><figcaption>Figure 1: Examining occupations heavily associated with embeddings of the words <em>he-she, </em>based on training from g2vNEWS text [1].</figcaption></figure> <p>In 2022, a study led by Caliskan at the University of Washington explored the gender biases within word embeddings, focusing on how models group words and concepts in relation to their meanings in the human language. Researchers conducted cluster analysis on the top words associated with gender and found a notable difference between socially “male” and “female” gendered topics. Top “male” concepts surrounded topics of leadership, technology, and physical strength–“engineering,” “CEO,” “sports.” Contrastingly, “female” concepts were less focused on positions and included terms like “beautiful,” “marriage,” “spa,” alongside more troubling associations with derogatory slurs and sexualized language [2].</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Yc3Z1MJ1scrdjnF3"><figcaption>Table 3: Top associated concepts of 1,000 female and 1,000 male associated words [2].</figcaption></figure> <p>These clusters reflected not only professional disparities but also broader societal biases that prioritize men in positions of power, reducing women to domestic or objectified roles. For example, although a word like beautiful is subjectively positive, it is discriminatory if it reduces women’s associations with other words, such as professional.</p> <h3><strong>Societal Implications</strong></h3> <p>Associating certain professions and activities with specific genders causes representational harm by reinforcing stereotypes about what men and women are capable of achieving. When these biased associations are embedded in widely used language models, they do more than reflect societal attitudes — they actively shape them, leading women and girls to internalize these stereotypes and narrow the scope of what they aspire to achieve. As AI systems become more pervasive, they risk perpetuating these biases, projecting limiting views onto future generations and further strengthening gender inequality in society.</p> <h3><strong>De-Biasing The Systems</strong></h3> <p>Debiasing these systems is a process that has been long sought after. The end goal is to remove the underlying prejudices these gender-neutral vectors possess while maintaining their semantic relationships. As a result, researchers have invented many mitigation techniques, with post-processing techniques being the most notable. Subspace is a specific method of post-processing that focuses on identifying the “bias direction” in the embedding space, analyzing the relationships between words known to exude bias–”men” , “women”, “he”, “she”. This technique aims to remove the gendered directionality from these word relationships, making them neutral [1]. In most cases, even if metrics showcase an adjustment to the bias, it is still present and recoverable. This underscores the critical need for further research in this area to foster ethical AI practices that do not discriminate against any groups.​ By advancing our understanding and methods to effectively eliminate bias, we can pave the way for more equitable and just artificial intelligence systems.</p> <p><strong>References</strong></p> <p>[1] Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings, Bolukbasi et al., 2016, <a href="https://arxiv.org/pdf/1607.06520.pdf" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/1607.06520.pdf</a></p> <p>[2]Semantics derived automatically from language corpora contain human-like biases, Caliskan et al., 2017, <a href="https://arxiv.org/pdf/2206.03390v1" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2206.03390v1</a></p> <p>[3]Semantic Feature Space, <a href="https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/tutorial.html" rel="external nofollow noopener" target="_blank">https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/tutorial.html</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=01b775062413" width="1" height="1" alt=""></p> </body></html>