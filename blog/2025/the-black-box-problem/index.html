<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*l3MEAQWBuOKfEo4O"><figcaption>Image from UM-Dearborn News</figcaption></figure> <p>By: Elizabeth Louie</p> <p>Additional Contributions: Sahana Narayan, Pearl Vishen</p> <h3>Introduction</h3> <p>As artificial intelligence continues to rapidly develop, it is becoming more and more integrated into important aspects of our daily lives. For example, AI is now widely used in the medical field to identify patterns in medical images, therefore detecting certain diseases and enabling early diagnosis (Blouin). However, it is important for humans to trust the outputs machine learning systems provide and Explainable AI (XAI) can help to do so.</p> <h3>The Black Box Problem</h3> <p>As AI constantly advances, it has become harder to understand and retrace how an algorithm creates its output. This inability to see how deep learning systems calculate processes of algorithms is called the “black box” problem. The same way humans have implicit knowledge, deep learning algorithms lose track of inputs that were used during algorithm training (Blouin). As AI has developed great accuracy by employing more complex algorithms, these contemporary systems can’t explain their decisions straightforwardly.</p> <p>Now that AI is being used in the medical field, for mental health resources, and for education, the “black box” problem can become a serious ethical issue. For instance, if an autonomous car hits a pedestrian, humans are unable to trace back to where the AI system failure occurred (Blouin). Explainable AI (XAI) focuses on ensuring a transparent understanding when utilizing AI systems, by clearly describing an AI model’s decisions and its expected impact while meeting regulatory standards.</p> <h3>Explainability Techniques</h3> <p>XAI is based on the concept of explainability. ScienceDirect, an online database of peer-reviewed articles, defines explainability as, “the process of elucidating or revealing the decision-making mechanisms of models.” There are a variety of techniques for explainability, providing insights to machine learning decisions.</p> <p>One is called a scoop-based explainer and uses feature importance analysis, a technique that calculates how much impact each input has on a machine learning model’s prediction of a target variable (Ali et. al). The analysis is categorized as local or global. The local method is limited to a single decision or instance with only one explanation whereas the global method provides rationale for the entire data set.</p> <p>A good example of a local method is LIME, which stands for local interpretable model-agnostic explanations. LIME alters original data points, feeds these back into the black box, and observes the output in response to the modified data (Dhinakaran). This strategy assigns weight to new data points as in some will matter more than others, determined by how close new data points are to the original. Ultimately, this creates a simpler model approximating the behavior of the original more complex model. The goal of LIME is to identify an interpretable model, something easier for humans to understand such as a binary tree where decision making is clear.</p> <h3>A Four Axes Model for Explainability</h3> <p>A research article published on ScienceDirect by Sajid Ali and other researchers, proposes a four axes methodology for explainability in deep neural networks. This framework analyzes and evaluates XAI by considering four distinct dimensions to allow for a multifaceted examination. In other words, this model looks at XAI from four different angles using a hierarchical categorization system to gain a comprehensive understanding. SceinceDirect proposes this model to “diagnose the training process and to refine the model for robustness and trustworthiness.” Each axis has various research questions to guide inquiry, as well as a taxonomy to classify categorization of concepts associated with each axis. All four axes are important for an adequate understanding of the explanation.</p> <p>The first axis is data explainability, which uses tools and various methods to summarize and analyze data to offer subsequent understanding of the data used to train AI models. This axis is important because the performance of an AI model is influenced by the characteristics of the data it’s trained on. Aspects of data explainability include comprehending knowledge graphs, data summarizing, exploratory data analysis, and any preprocessing or transformations applied (Ali et. al). Some of the research questions that ScienceDirect proposes to address this axis of explanation include: What sort of information do we have in the database? What can be inferred from this data? What are the most important portions of the data? (Ali et. al) These questions explore the dataset’s content, relevance, usability, and the axis’s role in improving model interpretability and performance. Data explainability offers insights to how open and understandable the AI model is to users, while focusing on the inner workings such as its decision-making processes providing overall transparency within the four axes model.</p> <p>The second axis is model explainability, which shows the internal structure and algorithms of an AI model, creating an understanding on how the model processes inputs to produce outputs. Model explainability focuses on interpretability (Ali et. al). ScienceDirect defines interpretability by saying it “enables developers to delve into the models decision making process, boosting confidence in understanding where the model gets its results.” This can involve selecting model types that are easier to interpret such as linear regression or decision trees as used in LIME. The importance of the model explainability axis is that it makes AI systems interpretable for humans. An example of this for a neural network might involve techniques that visualize which layers are responsible for certain types of information. Research questions guiding this axis include: What makes a parameter, objective, or action important to the system? When did the system examine a parameter, objective, or action and when did the model reject it? What are the consequences of making a difference decision or adjusting a parameter? (Ali et. al). These prompts look at how the AI model operates and what factors affect the model’s behavior.</p> <p>The next axis is post-hoc explainability, designed to elucidate significant features of an AI model using several kinds of explanation. Post-hoc explainability as described by ScienceDirect “refers to methods/algorithms that are used to explain AI model’s decisions.” The research questions relevant to evaluation post-hoc explainability include: What is the reason behind the model’s prediction? What was the reason for occurrence X? What variables have the most influence on the user’s decision? (Ali et. al). The overall purpose of this axis is to allow users to understand individual predictions without requiring full transparency into the model’s internals by interpreting the decision making process.</p> <p>The last axis, assessment of explanations, ensures that explanations are clear, accurate, and useful for different audiences. The criteria for assessment includes completeness, fidelity, and comprehensibility. The purpose of this last axis is to ensure meaningful and technically accurate explanations for XAI users. Altogether these axes support a robust approach to creating trustworthy AI systems.</p> <h3>Conclusion</h3> <p>Explainable AI helps us characterize model accuracy and transparency in areas that have previously been uninterpretable. As AI continues to develop in various areas in our lives ranging from search engine optimization to applications in the medical fields, advocating for transparency is crucial. Deep learning systems give rise to the black box problem when more sophisticated algorithms are employed, and the complex nature of these algorithms make it so these systems are unable to explain its decisions in a straightforward manner. This issue makes it even harder to trust the outputs of AI that we constantly rely on.</p> <p>Explainable AI provides a framework of transparency for AI users. As mentioned, there are many approaches to XAI techniques. A scoop based explainer categorizes feature importance analysis into either local or global analysis. Local methods are limited to a single explanation whereas the global method explains the entire data set. The previous example was LIME, which approximates the behavior of complex systems in order to provide an explanation. Researchers from ScienceDirect propose another technique for XAI involving a four axes framework to provide layered explanations of AI models. Using these various methods of explainable AI, we can put more faith into these complex algorithms and deep learning systems that are becoming increasingly prevalent in our society.</p> <h3><strong>References</strong></h3> <p>Ali, Sajid, et al. “Explainable Artificial Intelligence (XAI): What We Know and What Is Left to Attain Trustworthy Artificial Intelligence.” Information Fusion, Elsevier, 18 Apr. 2023, <a href="http://www.sciencedirect.com/science/article/pii/S1566253523001148." rel="external nofollow noopener" target="_blank">www.sciencedirect.com/science/article/pii/S1566253523001148.</a></p> <p>Blouin, Lou. “Ai’s Mysterious ‘black Box’ Problem, Explained.” Dearborn, umdearborn.edu/news/ais-mysterious-black-box-problem-explained. Accessed 26 Jan. 2025.</p> <p>Dhinakaran, Aparna. “What Are the Prevailing Explainability Methods?” Medium, Towards Data Science, 22 Dec. 2021, towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c40d3c6f26fe" width="1" height="1" alt=""></p> </body></html>