<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>By: Pedro Tajia, Aanchal Acharya</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/825/0*KxFxMv9jvTViNM8c"></figure> <p><strong><em>Figure 1:</em></strong><em> X is augmented to generate two kinds of examples v and v’ these new examples are imputed into the model f with parameters. The model outputs a vector that represents the input into a lower dimension. The model f is trained by using Self-Supervised learning to output a vector that contains rich representations.</em></p> <p>When working with artificial intelligence, the vast amount of data that is there brings with it many different types of data, as well. We must train models with data, and some training methods include supervised learning, unsupervised learning, reinforcement learning, and more. This article will discuss one specific branch of unsupervised learning, known as self-supervised learning. Self-supervised learning is a machine learning technique in which a model learns representations or features from unlabeled data by generating its own supervision signal. Another way to think about it is that it “fills in the blanks” of data on its own rather than humans having to do it manually. It allows training to take place efficiently on unlabeled data, which is very tedious and time consuming if done any other way.</p> <p><strong>What is it?</strong></p> <p>Self-supervised learning (SSL) is the process where the model is trained with unlabeled data by creating its own labels from the data itself, that is why it is called “self-supervised learning”. The use of supervised learning limits the ability of the model to understand the underlying patterns of the data, by labeling information the model is forced to learn the information in our way, instead of the model finding the meaning of the data by itself.</p> <p>Self-supervised learning is the phase where the model learns the structure and semantics of the data. Supervised Learning drives this initial knowledge, which is directed toward solving a specific task. Supervised learning forces the model to mold their knowledge into our perspective.</p> <p>Supervised learning is done by using label data to solve a task which serves as a guide to the pre-trained model from SSL toward learning a specific task, like predicting the name of a car based on images.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3xM-DnXBJahYrYwt"></figure> <p><strong><em>Figure 2: </em></strong><em>In the section on self-supervised learning, the network f task is to learn the semantics or the meaning of the inputs for example the texture, contours, shading, etc. Then supervised learning to fine-tune the network on label datasets. In this case, learn to predict the class probabilities of the Model of a car by giving an image. In this case, labeled data is used to guide the network f to predict class probabilities. For example, given an image of a car, the network is trained to classify the model of the”car” by recalling the previously learned features with the labeled category.</em></p> <p>The success of LLMs like ChatGPT, Gemini, LLaMa, etc comes from Self-Supervised learning. These models are trained to predict missing words in a text, and then compare the prediction with the original text; this process is called <strong>Masked Language Modeling (MLM). </strong>In this setup, the model learns the syntax and semantics of the text to do a good job of filling the “correct” words.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*tjWBaC0HiHad1ilw"></figure> <p><strong><em>Figure 3: </em></strong><em>This is a rough illustration of applying self-supervised learning in the training of a Large Language model (LLM). The first stage of training an LLM is using SSL to learn the syntax and semantics of words from a large corpus of text. In the training examples some words are masked or replaced, the task of the model is to reconstruct the text into its original state. When the model outputs diverge from the original text the error is propagated through the model to improve its predictions.</em></p> <p>There is a similar implementation on images where an image is corrupted and the model, like an autoencoder, tries to reconstruct the image to its original state; this is called <strong>denoising autoencoder.</strong></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*7X_FutOgSG0hnUM3"></figure> <p><strong><em>Figure 4:</em></strong><em> The task of the denoising autoencoder is by giving a corrupted image the model needs to reconstruct the image into the original image. The error is calculated by finding the mismatch between the reconstructed image and the original image, the error is propagated on the model to improve its predictions.</em></p> <p>There are many ways of applying SSL, now in these days it has been developing more sophisticated frameworks to archive state-of-the-art results. For a deeper dive into these concepts, consider reading <a href="https://sthalles.github.io/simple-self-supervised-learning/" rel="external nofollow noopener" target="_blank">Simple Self-Supervised Learning</a> and <a href="https://pedrotajia.com/2024/10/20/bootstrap-your-own-latent.html">Bootstrap Your Own Latent</a>.</p> <p><strong>Current applications</strong></p> <p>Self-supervised learning is a technique that can be applied in any field that utilizes artificial intelligence. There are currently many applications for self-supervised learning, ranging from areas such as finance to healthcare.</p> <p>The use of self-supervised learning in computer vision has helped professionals in various fields enhance their performances. It has trained models to detect abnormalities in medical images such as X-rays and CT scans. Autonomous vehicles have also been consistently trained with images and videos of road signs, lanes, and other foreign objects to help them recognize patterns and drive in a safe manner.</p> <p>Self-supervised learning is very useful in natural language processing, as well. It is the basis of many language models, such as BERT and GPT. These models are trained using large amounts of text to complete tasks such as generating text, answering questions, and much more. It is also used for sentiment analysis when it comes to comments on social media, customer reviews on a product, public opinion analysis in politics, and much more. Chatbots and virtual assistants use self-supervised learning to generate responses to be as human-like as possible.</p> <p>Models are trained with many different types of data when it comes to processing speech and audio. Training the model using self-supervised learning allows it to process speech and audio with higher accuracy. It can detect vocabulary from the choice of words in speeches, such as lectures and interviews, and transcribe it to text. It also can pick up on sentiment from a person’s voice in audios and correctly identify sounds in recordings.</p> <p>In the finance industry, self-supervised learning has been used to help detect fraudulent activity in transaction data by identifying unusual patterns. It also helps with early risk detection to help prevent the company from facing any major consequences by monitoring patterns and sentiment in the news, customer feedback, and more.</p> <p>Overall, self-supervised learning can be implemented in any field to enhance the results of individuals and help companies reach their goals faster and as efficiently as possible.</p> <p><strong>Benefits</strong></p> <p>There are many great attributes self-supervised learning brings to the table that other machine learning processes are unable to do themselves. When training data, we have to choose which data is best fit for the model. Unfortunately, a vast majority of data is unlabeled, which is not ideal for training a model unless it can be cleaned easily. Self-supervised learning acts as the key to being able to “unlock” the use of the unlabeled data, and utilize its potential. It allows us to use the readily available unlabeled data instead of looking for scarce, hard to find labeled data, which even when it is found, is not found in amounts as large as unlabeled data is.</p> <p>Unlabeled data provides us with an abundance of information, but that bounty of data is proportional to its inefficiency. The cleanup and management of this amount of data is extremely lengthy and prone to human error. Self-supervised learning saves that time spent in cleaning the data for humans and allows us to focus on other important responsibilities, such as analysis. It also improves data accuracy by preventing human errors such as incorrect data entry, which is very difficult to find and correct if it happens with datasets so large.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HceW6I-RRoLoGFLb"></figure> <p><strong><em>Figure 5: </em></strong><em>This is a representation of labeled (red dots) and unlabeled data (other dots). When a model is trained only on the labeled (red dots), the model will be limited to learning from a confined space. Instead, if the model is trained on unlabeled data (other dots) the model will have an enormous amount of knowledge and understanding about the data, then with that knowledge, it can guide it by using the label data toward a more specific area. The red dots serve as a guide for the other dots.</em></p> <p>While reducing time spent cleaning data and getting it ready for training, self-supervised learning also greatly reduces the money cost of the data cleaning. Manually cleaning data of this capacity requires a great amount of money and expertise to do so. Instead of paying humans to do this monotonous, yet very important, task for unprecedented amounts of time, the machine can do the cleaning itself while humans can dedicate their time towards tasks that cannot be accomplished by machines.</p> <p>Self-supervised learning is also known to be very scalable and adaptable across domains. It obtains this versatility because of its strong ability to find patterns in such severely unorganized data allows them to implement those patterns in any type of domain without too much customization. Its scalability ranges from any dataset size, and the same concepts can be applied to any dataset regardless of its magnitude.</p> <p><strong>How will it affect our future?</strong></p> <p>Even these days self-supervised learning has a great impact on artificial intelligence, like in the training of LLMs and computer vision models. As more development is toward improving these self-supervised learning frameworks the new model will achieve higher performance. As more people use technology more unlabeled data will be generated, and the improvement on these SSL frameworks will be able to use these data more efficiently and as a consequence will be a more useful model that will augment our capacities and improve human condition.</p> <p><strong>Conclusion</strong></p> <p>In conclusion, Self-Supervised learning is a framework designed to make a model or system learn from unlabeled. Self-supervised learning is a phase where the model learns general information about a topic and then is given to perform a general task by using Supervised learning where it uses label data.</p> <p>Work cited:</p> <ol> <li> <strong>Huyen, Chip.</strong> “RLHF: Reinforcement Learning from Human Feedback.” <em>huyenchip.com</em>, 2 May 2023. <a href="https://huyenchip.com/2023/05/02/rlhf.html" rel="external nofollow noopener" target="_blank">https://huyenchip.com/2023/05/02/rlhf.html</a> </li> <li> <strong>Chen, Ting, et al.</strong> “A Simple Framework for Contrastive Learning of Visual Representations.” <em>arXiv</em>, 11 Feb. 2020. <a href="https://arxiv.org/pdf/2002.05709" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2002.05709</a> </li> <li> <strong>Meta AI.</strong> “Self-Supervised Learning: The Dark Matter of Intelligence.” <em>ai.meta.com</em>, 4 Mar. 2021. <a href="https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/" rel="external nofollow noopener" target="_blank">https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/</a> </li> <li> <strong>Boesch, Gaudenz.</strong> “Self-Supervised Learning: Everything You Need to Know (2024).” <em>viso.ai</em>, 24 Oct. 2023. <a href="https://viso.ai/deep-learning/self-supervised-learning-for-computer-vision/" rel="external nofollow noopener" target="_blank">https://viso.ai/deep-learning/self-supervised-learning-for-computer-vision/</a> </li> <li> <strong>Bgn, Jonathan.</strong> “The Rise of Self-Supervised Learning.” <em>jonathanbgn.com</em>, 31 Dec. 2020. <a href="https://jonathanbgn.com/2020/12/31/self-supervised-learning.html" rel="external nofollow noopener" target="_blank">https://jonathanbgn.com/2020/12/31/self-supervised-learning.html</a> </li> <li> <strong>Neptune.ai.</strong> “Self-Supervised Learning and Its Applications.” <em>neptune.ai</em>, <a href="https://neptune.ai/blog/self-supervised-learning" rel="external nofollow noopener" target="_blank">https://neptune.ai/blog/self-supervised-learning</a> </li> <li> <strong>Tajia, Pedro.</strong> “Bootstrap Your Own Latent: Self-Supervised Learning Without Contrastive Learning.” <em>pedrotajia.com</em>, 20 Oct. 2024. <a href="https://pedrotajia.com/2024/10/20/bootstrap-your-own-latent.html">https://pedrotajia.com/2024/10/20/bootstrap-your-own-latent.html</a> </li> </ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5d00fa1c8b8e" width="1" height="1" alt=""></p> </body></html>